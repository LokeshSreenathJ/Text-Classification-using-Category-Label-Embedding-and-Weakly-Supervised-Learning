{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mf1FefzN17H",
        "outputId": "89b78744-250d-4e68-c5a3-1a1bfe4b4bc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloning Shang Jing's GitHub repository to extract the Auto Phrase code, which aids in extracting high-quality text for accurately capturing the context's essential words."
      ],
      "metadata": {
        "id": "aZMBhuCNBJnG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSxoM9uxnwpb",
        "outputId": "be6b82e7-4848-4ede-f20b-ea0872077406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'AutoPhrase'...\n",
            "remote: Enumerating objects: 967, done.\u001b[K\n",
            "remote: Counting objects: 100% (137/137), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 967 (delta 66), reused 124 (delta 60), pack-reused 830\u001b[K\n",
            "Receiving objects: 100% (967/967), 199.80 MiB | 10.98 MiB/s, done.\n",
            "Resolving deltas: 100% (438/438), done.\n",
            "Updating files: 100% (204/204), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/shangjingbo1226/AutoPhrase.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuXr6CEdN1_o",
        "outputId": "68da06d5-efe9-4679-c732-66262c8dc646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/AutoPhrase\n",
            "\u001b[32m===Compilation===\u001b[m\n",
            "\u001b[32m===Tokenization===\u001b[m\n",
            "\n",
            "real\t0m41.948s\n",
            "user\t1m3.242s\n",
            "sys\t0m1.482s\n",
            "Detected Language: EN\u001b[0K\n",
            "Current step: Tokenizing wikipedia phrases...\u001b[0K\n",
            "No provided expert labels.\u001b[0K\n",
            "\u001b[32m===AutoPhrasing===\u001b[m\n",
            "=== Current Settings ===\n",
            "Iterations = 2\n",
            "Minimum Support Threshold = 10\n",
            "Maximum Length Threshold = 6\n",
            "POS-Tagging Mode Disabled\n",
            "Discard Ratio = 0.050000\n",
            "Number of threads = 10\n",
            "Labeling Method = DPDN\n",
            "\tAuto labels from knowledge bases\n",
            "\tMax Positive Samples = -1\n",
            "=======\n",
            "Loading data...\n",
            "# of total tokens = 13325086\n",
            "max word token id = 119699\n",
            "# of documents = 50001\n",
            "# of distinct POS tags = 0\n",
            "Mining frequent phrases...\n",
            "selected MAGIC = 119701\n",
            "# of frequent phrases = 340609\n",
            "Extracting features...\n",
            "Constructing label pools...\n",
            "\tThe size of the positive pool = 18045\n",
            "\tThe size of the negative pool = 320290\n",
            "# truth patterns = 180094\n",
            "Estimating Phrase Quality...\n",
            "Segmenting...\n",
            "Rectifying features...\n",
            "Estimating Phrase Quality...\n",
            "Segmenting...\n",
            "Dumping results...\n",
            "Done.\n",
            "\n",
            "real\t4m57.746s\n",
            "user\t7m0.584s\n",
            "sys\t0m32.734s\n",
            "\u001b[32m===Saving Model and Results===\u001b[m\n",
            "\u001b[32m===Generating Output===\u001b[m\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/AutoPhrase/\n",
        "! bash auto_phrase.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the Yumeng Github for extracting the code for generating the Category based embeddings.\n",
        "\n",
        "\n",
        "Generating category-based embeddings offers several advantages:\n",
        "\n",
        "1.Improved classification: It enhances the model's ability to classify text into specific categories, enabling better accuracy in tasks like topic labeling or sentiment analysis.\n",
        "\n",
        "2.Contextual understanding: Category-based embeddings capture unique context and nuances within each category, providing more precise representations of text for different domains.\n",
        "\n",
        "3.Enhanced transfer learning: These embeddings can be used in various downstream NLP tasks, helping models transfer knowledge and perform better on specific categories, improving overall model performance."
      ],
      "metadata": {
        "id": "aJZN1ouOCvmp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31fxb9-HNKmw",
        "outputId": "e90629f6-4a0b-44f0-c8f1-c4e18f9a950f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/\n",
        "#! git clone https://github.com/yumeng5/CatE.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phrasal segmentation is performed on the datasets using high-quality text, and then the background and contextual words are jointly extracted and combined."
      ],
      "metadata": {
        "id": "mj_1iwYGEz6u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf9T1XY9Nl1x",
        "outputId": "7c012acb-78e9-4886-9803-db4988e64d34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/AutoPhrase\n",
            "\u001b[32m===Compilation===\u001b[m\n",
            "\u001b[32m===Tokenization===\u001b[m\n",
            "\n",
            "real\t0m18.162s\n",
            "user\t0m26.235s\n",
            "sys\t0m1.381s\n",
            "Detected Language: EN\u001b[0K\n",
            "\u001b[32m===Phrasal Segmentation===\u001b[m\n",
            "=== Current Settings ===\n",
            "Segmentation Model Path = model_Movies/DBLP/segmentation.model\n",
            "After the phrasal segmentation, only following phrases will be highlighted with <phrase> and </phrase>\n",
            "\tQ(multi-word phrases) >= 0.700000\n",
            "\tQ(single-word phrases) >= 1.000000\n",
            "=======\n",
            "Length penalty model loaded.\n",
            "\tpenalty = 199.805\n",
            "# of loaded patterns = 48912\n",
            "# of loaded truth patterns = 198139\n",
            "Phrasal segmentation finished.\n",
            "   # of total highlighted quality phrases = 1173121\n",
            "   # of total processed sentences = 1581322\n",
            "   avg highlights per sentence = 0.741861\n",
            "\n",
            "real\t0m19.100s\n",
            "user\t0m17.101s\n",
            "sys\t0m0.397s\n",
            "\u001b[32m===Generating Output===\u001b[m\n",
            "\n",
            "real\t0m20.819s\n",
            "user\t0m17.144s\n",
            "sys\t0m1.055s\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/AutoPhrase/\n",
        "! bash phrasal_segmentation.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process the segmented words by converting them to lowercase and join the main word with background words using underscores to ensure accurate context understanding by the embedding generator."
      ],
      "metadata": {
        "id": "F0Lhokx2FueM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBAm70lJNl5V",
        "outputId": "9bbee6f6-8e8b-4c3b-f8c2-151f98844d5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127600it [00:02, 44581.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phrase segmented corpus written to ./AutoPhrase/model_News/DBLP/text.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50001/50001 [01:53<00:00, 442.09it/s]\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from nltk import word_tokenize\n",
        "import re\n",
        "\n",
        "def phrase_process():\n",
        "\tf = open(os.path.join('AutoPhrase', 'model_News', \"DBLP\" , 'segmentation.txt'))\n",
        "\tg = open(args.out_file, 'w')\n",
        "\tfor line in tqdm(f):\n",
        "\t\tdoc = ''\n",
        "\t\ttemp = re.split(r'<phrase_Q=\\d\\.\\d+>', line)\n",
        "\t\tfor seg in temp:\n",
        "\t\t\ttemp2 = seg.split('</phrase>')\n",
        "\t\t\tif len(temp2) > 1:\n",
        "\t\t\t\tdoc += (\"_\").join(temp2[0].split(\" \")) + temp2[1]\n",
        "\t\t\telse:\n",
        "\t\t\t\tdoc += temp2[0]\n",
        "\t\tg.write(doc.strip()+'\\n')\n",
        "\tprint(\"Phrase segmented corpus written to {}\".format(args.out_file))\n",
        "\treturn\n",
        "def preprocess():\n",
        "\tf = open(os.path.join(\"AutoPhrase\",\"model_Movies/DBLP\",\"text.txt\"))\n",
        "\tdocs = f.readlines()\n",
        "\tf_out = open(args.out_file, 'w')\n",
        "\tfor doc in tqdm(docs):\n",
        "\t\tf_out.write(' '.join([w.lower() for w in word_tokenize(doc.strip())]) + '\\n')\n",
        "\treturn\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "\n",
        "\tparser = argparse.ArgumentParser(description='main', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "\tparser.add_argument('--mode', type=int)\n",
        "\tparser.add_argument('--dataset', default=\"News\")\n",
        "\tparser.add_argument('--in_file', default='segmentation.txt')\n",
        "\tparser.add_argument('--out_file', default='./AutoPhrase/model_News/DBLP/text.txt')\n",
        "\targs, _ = parser.parse_known_args()\n",
        "\n",
        "\n",
        "phrase_process()\n",
        "preprocess()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKqUsOxgHQIr",
        "outputId": "0df70a3e-b0b9-48df-8a28-5b346a740ba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CatE\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/CatE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff7hPrN6HCFU"
      },
      "outputs": [],
      "source": [
        "!chmod +x ./src/cate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmx_ITfrHQK-",
        "outputId": "4dece4d0-54d5-4974-fb5f-ddd85b7df1f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "make: 'cate' is up to date.\n",
            "Starting training using file ./datasets/News/News_processed.txt\n",
            "Training with specificity; Specificity values output to file ./datasets/News/emb_category_spec.txt\n",
            "Reading topics from file ./datasets/News/category.txt\n",
            "Vocab size: 50110\n",
            "Words in train file: 12837739\n",
            "Loading embedding from file word2vec_100.txt\n",
            "In vocab: 35656\n",
            "Read 4 topics\n",
            "politics\t\n",
            "sports\t\n",
            "business\t\n",
            "technology\t\n",
            "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
            "Alpha: 0.024969  Progress: 0.13%  Words/thread/sec: 42.63k  ^C\n"
          ]
        }
      ],
      "source": [
        "! bash run.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdje7oT2HQM-",
        "outputId": "4e7f0ab2-b880-487e-e7e1-5fdb5ce472fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/WeSTClass\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/WeSTClass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5vf6v3yHQO5",
        "outputId": "cd66e97d-55f3-4370-cc57-4a0254490e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'WeSTClass'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Total 75 (delta 0), reused 0 (delta 0), pack-reused 75\u001b[K\n",
            "Receiving objects: 100% (75/75), 61.32 MiB | 11.68 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n",
            "Updating files: 100% (20/20), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/yumeng5/WeSTClass.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nfnuZ2yHQSY",
        "outputId": "31ae3617-f271-4f83-eb51-407241aca055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.8.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.3.2)\n",
            "Collecting sklearn (from -r requirements.txt (line 4))\n",
            "  Downloading sklearn-0.0.post10.tar.gz (3.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "! pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghxD3KROQZvs",
        "outputId": "5b0da819-8381-4cd1-8a49-b8617a970c46"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBL0UQe3O1Yp",
        "outputId": "a0d86a1d-f9e8-4256-995f-3ad334377b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spherecluster\n",
            "  Downloading spherecluster-0.1.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spherecluster) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from spherecluster) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.10/dist-packages (from spherecluster) (1.2.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from spherecluster) (7.4.3)\n",
            "Collecting nose (from spherecluster)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->spherecluster) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->spherecluster) (3.2.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->spherecluster) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->spherecluster) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->spherecluster) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->spherecluster) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->spherecluster) (2.0.1)\n",
            "Installing collected packages: nose, spherecluster\n",
            "Successfully installed nose-1.3.7 spherecluster-0.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install spherecluster\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO40HsW0QqPP",
        "outputId": "cb7921fa-47ab-40c6-879b-a32efb95fdce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster._kmeans import (\n",
        "    _check_sample_weight,\n",
        "    _labels_inertia,\n",
        "    _tolerance,\n",
        ")\n",
        "from sklearn.utils import check_array, check_random_state\n",
        "from sklearn.utils.validation import _num_samples\n",
        "from sklearn.cluster import _kmeans\n",
        "from sklearn.preprocessing import normalize\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.utils.extmath import row_norms, squared_norm\n",
        "\n",
        "\n",
        "def _spherical_kmeans_single_lloyd(\n",
        "    X,\n",
        "    n_clusters,\n",
        "    sample_weight=None,\n",
        "    max_iter=300,\n",
        "    init=\"k-means++\",\n",
        "    verbose=False,\n",
        "    x_squared_norms=None,\n",
        "    random_state=None,\n",
        "    tol=1e-4,\n",
        "    precompute_distances=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Modified from sklearn.cluster.k_means_.k_means_single_lloyd.\n",
        "    \"\"\"\n",
        "    random_state = check_random_state(random_state)\n",
        "\n",
        "    sample_weight = _check_sample_weight(X, sample_weight)\n",
        "\n",
        "    best_labels, best_inertia, best_centers = None, None, None\n",
        "\n",
        "    # init\n",
        "    centers = KMeans._init_centroids(\n",
        "        X, n_clusters, init, random_state=random_state, x_squared_norms=x_squared_norms\n",
        "    )\n",
        "    if verbose:\n",
        "        print(\"Initialization complete\")\n",
        "\n",
        "    # Allocate memory to store the distances for each sample to its\n",
        "    # closer center for reallocation in case of ties\n",
        "    distances = np.zeros(shape=(X.shape[0],), dtype=X.dtype)\n",
        "\n",
        "    # iterations\n",
        "    for i in range(max_iter):\n",
        "        centers_old = centers.copy()\n",
        "\n",
        "        # labels assignment\n",
        "        # TODO: _labels_inertia should be done with cosine distance\n",
        "        #       since ||a - b|| = 2(1 - cos(a,b)) when a,b are unit normalized\n",
        "        #       this doesn't really matter.\n",
        "        labels, inertia = _labels_inertia(\n",
        "            X,\n",
        "            sample_weight,\n",
        "            x_squared_norms,\n",
        "            centers,\n",
        "            precompute_distances=precompute_distances,\n",
        "            distances=distances,\n",
        "        )\n",
        "\n",
        "        # computation of the means\n",
        "        if sp.issparse(X):\n",
        "            centers = _k_means._centers_sparse(\n",
        "                X, sample_weight, labels, n_clusters, distances\n",
        "            )\n",
        "        else:\n",
        "            centers = _k_means._centers_dense(\n",
        "                X, sample_weight, labels, n_clusters, distances\n",
        "            )\n",
        "\n",
        "        # l2-normalize centers (this is the main contibution here)\n",
        "        centers = normalize(centers)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Iteration %2d, inertia %.3f\" % (i, inertia))\n",
        "\n",
        "        if best_inertia is None or inertia < best_inertia:\n",
        "            best_labels = labels.copy()\n",
        "            best_centers = centers.copy()\n",
        "            best_inertia = inertia\n",
        "\n",
        "        center_shift_total = squared_norm(centers_old - centers)\n",
        "        if center_shift_total <= tol:\n",
        "            if verbose:\n",
        "                print(\n",
        "                    \"Converged at iteration %d: \"\n",
        "                    \"center shift %e within tolerance %e\" % (i, center_shift_total, tol)\n",
        "                )\n",
        "            break\n",
        "\n",
        "    if center_shift_total > 0:\n",
        "        # rerun E-step in case of non-convergence so that predicted labels\n",
        "        # match cluster centers\n",
        "        best_labels, best_inertia = _labels_inertia(\n",
        "            X,\n",
        "            sample_weight,\n",
        "            x_squared_norms,\n",
        "            best_centers,\n",
        "            precompute_distances=precompute_distances,\n",
        "            distances=distances,\n",
        "        )\n",
        "\n",
        "    return best_labels, best_inertia, best_centers, i + 1\n",
        "\n",
        "\n",
        "def spherical_k_means(\n",
        "    X,\n",
        "    n_clusters,\n",
        "    sample_weight=None,\n",
        "    init=\"k-means++\",\n",
        "    n_init=10,\n",
        "    max_iter=300,\n",
        "    verbose=False,\n",
        "    tol=1e-4,\n",
        "    random_state=None,\n",
        "    copy_x=True,\n",
        "    n_jobs=1,\n",
        "    algorithm=\"auto\",\n",
        "    return_n_iter=False,\n",
        "):\n",
        "    \"\"\"Modified from sklearn.cluster.k_means_.k_means.\n",
        "    \"\"\"\n",
        "    if n_init <= 0:\n",
        "        raise ValueError(\n",
        "            \"Invalid number of initializations.\"\n",
        "            \" n_init=%d must be bigger than zero.\" % n_init\n",
        "        )\n",
        "    random_state = check_random_state(random_state)\n",
        "\n",
        "    if max_iter <= 0:\n",
        "        raise ValueError(\n",
        "            \"Number of iterations should be a positive number,\"\n",
        "            \" got %d instead\" % max_iter\n",
        "        )\n",
        "\n",
        "    best_inertia = np.infty\n",
        "    # avoid forcing order when copy_x=False\n",
        "    order = \"C\" if copy_x else None\n",
        "    X = check_array(\n",
        "        X, accept_sparse=\"csr\", dtype=[np.float64, np.float32], order=order, copy=copy_x\n",
        "    )\n",
        "    # verify that the number of samples given is larger than k\n",
        "    if _num_samples(X) < n_clusters:\n",
        "        raise ValueError(\n",
        "            \"n_samples=%d should be >= n_clusters=%d\" % (_num_samples(X), n_clusters)\n",
        "        )\n",
        "    tol = _tolerance(X, tol)\n",
        "\n",
        "    if hasattr(init, \"__array__\"):\n",
        "        init = check_array(init, dtype=X.dtype.type, order=\"C\", copy=True)\n",
        "        KMeans._validate_center_shape(X, n_clusters, init)\n",
        "\n",
        "\n",
        "        if n_init != 1:\n",
        "            warnings.warn(\n",
        "                \"Explicit initial center position passed: \"\n",
        "                \"performing only one init in k-means instead of n_init=%d\" % n_init,\n",
        "                RuntimeWarning,\n",
        "                stacklevel=2,\n",
        "            )\n",
        "            n_init = 1\n",
        "\n",
        "    # precompute squared norms of data points\n",
        "    x_squared_norms = row_norms(X, squared=True)\n",
        "\n",
        "    if n_jobs == 1:\n",
        "        # For a single thread, less memory is needed if we just store one set\n",
        "        # of the best results (as opposed to one set per run per thread).\n",
        "        for it in range(n_init):\n",
        "            # run a k-means once\n",
        "            labels, inertia, centers, n_iter_ = _spherical_kmeans_single_lloyd(\n",
        "                X,\n",
        "                n_clusters,\n",
        "                sample_weight,\n",
        "                max_iter=max_iter,\n",
        "                init=init,\n",
        "                verbose=verbose,\n",
        "                tol=tol,\n",
        "                x_squared_norms=x_squared_norms,\n",
        "                random_state=random_state,\n",
        "            )\n",
        "\n",
        "            # determine if these results are the best so far\n",
        "            if best_inertia is None or inertia < best_inertia:\n",
        "                best_labels = labels.copy()\n",
        "                best_centers = centers.copy()\n",
        "                best_inertia = inertia\n",
        "                best_n_iter = n_iter_\n",
        "    else:\n",
        "        # parallelisation of k-means runs\n",
        "        seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\n",
        "        results = Parallel(n_jobs=n_jobs, verbose=0)(\n",
        "            delayed(_spherical_kmeans_single_lloyd)(\n",
        "                X,\n",
        "                n_clusters,\n",
        "                sample_weight,\n",
        "                max_iter=max_iter,\n",
        "                init=init,\n",
        "                verbose=verbose,\n",
        "                tol=tol,\n",
        "                x_squared_norms=x_squared_norms,\n",
        "                # Change seed to ensure variety\n",
        "                random_state=seed,\n",
        "            )\n",
        "            for seed in seeds\n",
        "        )\n",
        "\n",
        "        # Get results with the lowest inertia\n",
        "        labels, inertia, centers, n_iters = zip(*results)\n",
        "        best = np.argmin(inertia)\n",
        "        best_labels = labels[best]\n",
        "        best_inertia = inertia[best]\n",
        "        best_centers = centers[best]\n",
        "        best_n_iter = n_iters[best]\n",
        "\n",
        "    if return_n_iter:\n",
        "        return best_centers, best_labels, best_inertia, best_n_iter\n",
        "    else:\n",
        "        return best_centers, best_labels, best_inertia\n",
        "\n",
        "\n",
        "class SphericalKMeans(KMeans):\n",
        "    \"\"\"Spherical K-Means clustering\n",
        "\n",
        "    Modfication of sklearn.cluster.KMeans where cluster centers are normalized\n",
        "    (projected onto the sphere) in each iteration.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    n_clusters : int, optional, default: 8\n",
        "        The number of clusters to form as well as the number of\n",
        "        centroids to generate.\n",
        "\n",
        "    max_iter : int, default: 300\n",
        "        Maximum number of iterations of the k-means algorithm for a\n",
        "        single run.\n",
        "\n",
        "    n_init : int, default: 10\n",
        "        Number of time the k-means algorithm will be run with different\n",
        "        centroid seeds. The final results will be the best output of\n",
        "        n_init consecutive runs in terms of inertia.\n",
        "\n",
        "    init : {'k-means++', 'random' or an ndarray}\n",
        "        Method for initialization, defaults to 'k-means++':\n",
        "        'k-means++' : selects initial cluster centers for k-mean\n",
        "        clustering in a smart way to speed up convergence. See section\n",
        "        Notes in k_init for more details.\n",
        "        'random': choose k observations (rows) at random from data for\n",
        "        the initial centroids.\n",
        "        If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
        "        and gives the initial centers.\n",
        "\n",
        "    tol : float, default: 1e-4\n",
        "        Relative tolerance with regards to inertia to declare convergence\n",
        "\n",
        "    n_jobs : int\n",
        "        The number of jobs to use for the computation. This works by computing\n",
        "        each of the n_init runs in parallel.\n",
        "        If -1 all CPUs are used. If 1 is given, no parallel computing code is\n",
        "        used at all, which is useful for debugging. For n_jobs below -1,\n",
        "        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one\n",
        "        are used.\n",
        "\n",
        "    random_state : integer or numpy.RandomState, optional\n",
        "        The generator used to initialize the centers. If an integer is\n",
        "        given, it fixes the seed. Defaults to the global numpy random\n",
        "        number generator.\n",
        "\n",
        "    verbose : int, default 0\n",
        "        Verbosity mode.\n",
        "\n",
        "    copy_x : boolean, default True\n",
        "        When pre-computing distances it is more numerically accurate to center\n",
        "        the data first.  If copy_x is True, then the original data is not\n",
        "        modified.  If False, the original data is modified, and put back before\n",
        "        the function returns, but small numerical differences may be introduced\n",
        "        by subtracting and then adding the data mean.\n",
        "\n",
        "    normalize : boolean, default True\n",
        "        Normalize the input to have unnit norm.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "\n",
        "    cluster_centers_ : array, [n_clusters, n_features]\n",
        "        Coordinates of cluster centers\n",
        "\n",
        "    labels_ :\n",
        "        Labels of each point\n",
        "\n",
        "    inertia_ : float\n",
        "        Sum of distances of samples to their closest cluster center.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_clusters=8,\n",
        "        init=\"k-means++\",\n",
        "        n_init=10,\n",
        "        max_iter=300,\n",
        "        tol=1e-4,\n",
        "        n_jobs=1,\n",
        "        verbose=0,\n",
        "        random_state=None,\n",
        "        copy_x=True,\n",
        "        normalize=True,\n",
        "    ):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.init = init\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.n_init = n_init\n",
        "        self.verbose = verbose\n",
        "        self.random_state = random_state\n",
        "        self.copy_x = copy_x\n",
        "        self.n_jobs = n_jobs\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def fit(self, X, y=None, sample_weight=None):\n",
        "        \"\"\"Compute k-means clustering.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
        "\n",
        "        y : Ignored\n",
        "            not used, present here for API consistency by convention.\n",
        "\n",
        "        sample_weight : array-like, shape (n_samples,), optional\n",
        "            The weights for each observation in X. If None, all observations\n",
        "            are assigned equal weight (default: None)\n",
        "        \"\"\"\n",
        "        if self.normalize:\n",
        "            X = normalize(X)\n",
        "\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        # TODO: add check that all data is unit-normalized\n",
        "\n",
        "        self.cluster_centers_, self.labels_, self.inertia_, self.n_iter_ = spherical_k_means(\n",
        "            X,\n",
        "            n_clusters=self.n_clusters,\n",
        "            sample_weight=sample_weight,\n",
        "            init=self.init,\n",
        "            n_init=self.n_init,\n",
        "            max_iter=self.max_iter,\n",
        "            verbose=self.verbose,\n",
        "            tol=self.tol,\n",
        "            random_state=random_state,\n",
        "            copy_x=self.copy_x,\n",
        "            n_jobs=self.n_jobs,\n",
        "            return_n_iter=True,\n",
        "        )\n",
        "\n",
        "        return self\n"
      ],
      "metadata": {
        "id": "QrObIo3aH7Q4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from scipy.special import iv  # modified Bessel function of first kind, I_v\n",
        "from numpy import i0  # modified Bessel function of first kind order 0, I_0\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClusterMixin, TransformerMixin\n",
        "from sklearn.cluster._kmeans import (\n",
        "    _check_sample_weight,\n",
        "    _labels_inertia,\n",
        "    _tolerance,\n",
        ")\n",
        "from sklearn.utils.validation import FLOAT_DTYPES\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.utils import check_array, check_random_state, as_float_array\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.utils.extmath import squared_norm\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from sklearn.cluster import _kmeans\n",
        "\n",
        "\n",
        "\n",
        "MAX_CONTENTRATION = 1e10\n",
        "\n",
        "\n",
        "def _inertia_from_labels(X, centers, labels):\n",
        "    \"\"\"Compute inertia with cosine distance using known labels.\n",
        "    \"\"\"\n",
        "    n_examples, n_features = X.shape\n",
        "    inertia = np.zeros((n_examples,))\n",
        "    for ee in range(n_examples):\n",
        "        inertia[ee] = 1 - X[ee, :].dot(centers[int(labels[ee]), :].T)\n",
        "\n",
        "    return np.sum(inertia)\n",
        "\n",
        "\n",
        "def _labels_inertia(X, centers):\n",
        "    \"\"\"Compute labels and inertia with cosine distance.\n",
        "    \"\"\"\n",
        "    n_examples, n_features = X.shape\n",
        "    n_clusters, n_features = centers.shape\n",
        "\n",
        "    labels = np.zeros((n_examples,))\n",
        "    inertia = np.zeros((n_examples,))\n",
        "\n",
        "    for ee in range(n_examples):\n",
        "        dists = np.zeros((n_clusters,))\n",
        "        for cc in range(n_clusters):\n",
        "            dists[cc] = 1 - X[ee, :].dot(centers[cc, :].T)\n",
        "\n",
        "        labels[ee] = np.argmin(dists)\n",
        "        inertia[ee] = dists[int(labels[ee])]\n",
        "\n",
        "    return labels, np.sum(inertia)\n",
        "\n",
        "\n",
        "def _vmf_log(X, kappa, mu):\n",
        "    \"\"\"Computs log(vMF(X, kappa, mu)) using built-in numpy/scipy Bessel\n",
        "    approximations.\n",
        "\n",
        "    Works well on small kappa and mu.\n",
        "    \"\"\"\n",
        "    n_examples, n_features = X.shape\n",
        "    return np.log(_vmf_normalize(kappa, n_features) * np.exp(kappa * X.dot(mu).T))\n",
        "\n",
        "\n",
        "def _vmf_normalize(kappa, dim):\n",
        "    \"\"\"Compute normalization constant using built-in numpy/scipy Bessel\n",
        "    approximations.\n",
        "\n",
        "    Works well on small kappa and mu.\n",
        "    \"\"\"\n",
        "    num = np.power(kappa, dim / 2. - 1.)\n",
        "\n",
        "    if dim / 2. - 1. < 1e-15:\n",
        "        denom = np.power(2. * np.pi, dim / 2.) * i0(kappa)\n",
        "    else:\n",
        "        denom = np.power(2. * np.pi, dim / 2.) * iv(dim / 2. - 1., kappa)\n",
        "\n",
        "    if np.isinf(num):\n",
        "        raise ValueError(\"VMF scaling numerator was inf.\")\n",
        "\n",
        "    if np.isinf(denom):\n",
        "        raise ValueError(\"VMF scaling denominator was inf.\")\n",
        "\n",
        "    if np.abs(denom) < 1e-15:\n",
        "        raise ValueError(\"VMF scaling denominator was 0.\")\n",
        "\n",
        "    return num / denom\n",
        "\n",
        "\n",
        "def _log_H_asymptotic(nu, kappa):\n",
        "    \"\"\"Compute the Amos-type upper bound asymptotic approximation on H where\n",
        "    log(H_\\nu)(\\kappa) = \\int_0^\\kappa R_\\nu(t) dt.\n",
        "\n",
        "    See \"lH_asymptotic <-\" in movMF.R and utility function implementation notes\n",
        "    from https://cran.r-project.org/web/packages/movMF/index.html\n",
        "    \"\"\"\n",
        "    beta = np.sqrt((nu + 0.5) ** 2)\n",
        "    kappa_l = np.min([kappa, np.sqrt((3. * nu + 11. / 2.) * (nu + 3. / 2.))])\n",
        "    return _S(kappa, nu + 0.5, beta) + (\n",
        "        _S(kappa_l, nu, nu + 2.) - _S(kappa_l, nu + 0.5, beta)\n",
        "    )\n",
        "\n",
        "\n",
        "def _S(kappa, alpha, beta):\n",
        "    \"\"\"Compute the antiderivative of the Amos-type bound G on the modified\n",
        "    Bessel function ratio.\n",
        "\n",
        "    Note:  Handles scalar kappa, alpha, and beta only.\n",
        "\n",
        "    See \"S <-\" in movMF.R and utility function implementation notes from\n",
        "    https://cran.r-project.org/web/packages/movMF/index.html\n",
        "    \"\"\"\n",
        "    kappa = 1. * np.abs(kappa)\n",
        "    alpha = 1. * alpha\n",
        "    beta = 1. * np.abs(beta)\n",
        "    a_plus_b = alpha + beta\n",
        "    u = np.sqrt(kappa ** 2 + beta ** 2)\n",
        "    if alpha == 0:\n",
        "        alpha_scale = 0\n",
        "    else:\n",
        "        alpha_scale = alpha * np.log((alpha + u) / a_plus_b)\n",
        "\n",
        "    return u - beta - alpha_scale\n",
        "\n",
        "\n",
        "def _vmf_log_asymptotic(X, kappa, mu):\n",
        "    \"\"\"Compute log(f(x|theta)) via Amos approximation\n",
        "\n",
        "        log(f(x|theta)) = theta' x - log(H_{d/2-1})(\\|theta\\|)\n",
        "\n",
        "    where theta = kappa * X, \\|theta\\| = kappa.\n",
        "\n",
        "    Computing _vmf_log helps with numerical stability / loss of precision for\n",
        "    for large values of kappa and n_features.\n",
        "\n",
        "    See utility function implementation notes in movMF.R from\n",
        "    https://cran.r-project.org/web/packages/movMF/index.html\n",
        "    \"\"\"\n",
        "    n_examples, n_features = X.shape\n",
        "    log_vfm = kappa * X.dot(mu).T + -_log_H_asymptotic(n_features / 2. - 1., kappa)\n",
        "\n",
        "    return log_vfm\n",
        "\n",
        "\n",
        "def _log_likelihood(X, centers, weights, concentrations):\n",
        "    if len(np.shape(X)) != 2:\n",
        "        X = X.reshape((1, len(X)))\n",
        "\n",
        "    n_examples, n_features = np.shape(X)\n",
        "    n_clusters, _ = centers.shape\n",
        "\n",
        "    if n_features <= 50:  # works up to about 50 before numrically unstable\n",
        "        vmf_f = _vmf_log\n",
        "    else:\n",
        "        vmf_f = _vmf_log_asymptotic\n",
        "\n",
        "    f_log = np.zeros((n_clusters, n_examples))\n",
        "    for cc in range(n_clusters):\n",
        "        f_log[cc, :] = vmf_f(X, concentrations[cc], centers[cc, :])\n",
        "\n",
        "    posterior = np.zeros((n_clusters, n_examples))\n",
        "    weights_log = np.log(weights)\n",
        "    posterior = np.tile(weights_log.T, (n_examples, 1)).T + f_log\n",
        "    for ee in range(n_examples):\n",
        "        posterior[:, ee] = np.exp(posterior[:, ee] - logsumexp(posterior[:, ee]))\n",
        "\n",
        "    return posterior\n",
        "\n",
        "\n",
        "def _init_unit_centers(X, n_clusters, random_state, init):\n",
        "    \"\"\"Initializes unit norm centers.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
        "\n",
        "    n_clusters : int, optional, default: 8\n",
        "        The number of clusters to form as well as the number of\n",
        "        centroids to generate.\n",
        "\n",
        "    random_state : integer or numpy.RandomState, optional\n",
        "        The generator used to initialize the centers. If an integer is\n",
        "        given, it fixes the seed. Defaults to the global numpy random\n",
        "        number generator.\n",
        "\n",
        "    init:  (string) one of\n",
        "        k-means++ : uses sklearn k-means++ initialization algorithm\n",
        "        spherical-k-means : use centroids from one pass of spherical k-means\n",
        "        random : random unit norm vectors\n",
        "        random-orthonormal : random orthonormal vectors\n",
        "        If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
        "        and gives the initial centers.\n",
        "    \"\"\"\n",
        "    n_examples, n_features = np.shape(X)\n",
        "    if isinstance(init, np.ndarray):\n",
        "        n_init_clusters, n_init_features = init.shape\n",
        "        assert n_init_clusters == n_clusters\n",
        "        assert n_init_features == n_features\n",
        "\n",
        "        # ensure unit normed centers\n",
        "        centers = init\n",
        "        for cc in range(n_clusters):\n",
        "            centers[cc, :] = centers[cc, :] / np.linalg.norm(centers[cc, :])\n",
        "\n",
        "        return centers\n",
        "\n",
        "    elif init == \"spherical-k-means\":\n",
        "        labels, inertia, centers, iters = spherical_kmeans._spherical_kmeans_single_lloyd(\n",
        "            X, n_clusters, x_squared_norms=np.ones((n_examples,)), init=\"k-means++\"\n",
        "        )\n",
        "\n",
        "        return centers\n",
        "\n",
        "    elif init == \"random\":\n",
        "        centers = np.random.randn(n_clusters, n_features)\n",
        "        for cc in range(n_clusters):\n",
        "            centers[cc, :] = centers[cc, :] / np.linalg.norm(centers[cc, :])\n",
        "\n",
        "        return centers\n",
        "\n",
        "    elif init == \"k-means++\":\n",
        "        centers = _init_centroids(\n",
        "            X,\n",
        "            n_clusters,\n",
        "            \"k-means++\",\n",
        "            random_state=random_state,\n",
        "            x_squared_norms=np.ones((n_examples,)),\n",
        "        )\n",
        "\n",
        "        for cc in range(n_clusters):\n",
        "            centers[cc, :] = centers[cc, :] / np.linalg.norm(centers[cc, :])\n",
        "\n",
        "        return centers\n",
        "\n",
        "    elif init == \"random-orthonormal\":\n",
        "        centers = np.random.randn(n_clusters, n_features)\n",
        "        q, r = np.linalg.qr(centers.T, mode=\"reduced\")\n",
        "\n",
        "        return q.T\n",
        "\n",
        "    elif init == \"random-class\":\n",
        "        centers = np.zeros((n_clusters, n_features))\n",
        "        for cc in range(n_clusters):\n",
        "            while np.linalg.norm(centers[cc, :]) == 0:\n",
        "                labels = np.random.randint(0, n_clusters, n_examples)\n",
        "                centers[cc, :] = X[labels == cc, :].sum(axis=0)\n",
        "\n",
        "        for cc in range(n_clusters):\n",
        "            centers[cc, :] = centers[cc, :] / np.linalg.norm(centers[cc, :])\n",
        "\n",
        "        return centers\n",
        "\n",
        "\n",
        "def _expectation(X, centers, weights, concentrations, posterior_type=\"soft\"):\n",
        "    \"\"\"Compute the log-likelihood of each datapoint being in each cluster.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    centers (mu) : array, [n_centers x n_features]\n",
        "    weights (alpha) : array, [n_centers, ] (alpha)\n",
        "    concentrations (kappa) : array, [n_centers, ]\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    posterior : array, [n_centers, n_examples]\n",
        "    \"\"\"\n",
        "    n_examples, n_features = np.shape(X)\n",
        "    n_clusters, _ = centers.shape\n",
        "\n",
        "    if n_features <= 50:  # works up to about 50 before numrically unstable\n",
        "        vmf_f = _vmf_log\n",
        "    else:\n",
        "        vmf_f = _vmf_log_asymptotic\n",
        "\n",
        "    f_log = np.zeros((n_clusters, n_examples))\n",
        "    for cc in range(n_clusters):\n",
        "        f_log[cc, :] = vmf_f(X, concentrations[cc], centers[cc, :])\n",
        "\n",
        "    posterior = np.zeros((n_clusters, n_examples))\n",
        "    if posterior_type == \"soft\":\n",
        "        weights_log = np.log(weights)\n",
        "        posterior = np.tile(weights_log.T, (n_examples, 1)).T + f_log\n",
        "        for ee in range(n_examples):\n",
        "            posterior[:, ee] = np.exp(posterior[:, ee] - logsumexp(posterior[:, ee]))\n",
        "\n",
        "    elif posterior_type == \"hard\":\n",
        "        weights_log = np.log(weights)\n",
        "        weighted_f_log = np.tile(weights_log.T, (n_examples, 1)).T + f_log\n",
        "        for ee in range(n_examples):\n",
        "            posterior[np.argmax(weighted_f_log[:, ee]), ee] = 1.0\n",
        "\n",
        "    return posterior\n",
        "\n",
        "\n",
        "def _maximization(X, posterior, force_weights=None):\n",
        "    \"\"\"Estimate new centers, weights, and concentrations from\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    posterior : array, [n_centers, n_examples]\n",
        "        The posterior matrix from the expectation step.\n",
        "\n",
        "    force_weights : None or array, [n_centers, ]\n",
        "        If None is passed, will estimate weights.\n",
        "        If an array is passed, will use instead of estimating.\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    centers (mu) : array, [n_centers x n_features]\n",
        "    weights (alpha) : array, [n_centers, ] (alpha)\n",
        "    concentrations (kappa) : array, [n_centers, ]\n",
        "    \"\"\"\n",
        "    n_examples, n_features = X.shape\n",
        "    n_clusters, n_examples = posterior.shape\n",
        "    concentrations = np.zeros((n_clusters,))\n",
        "    centers = np.zeros((n_clusters, n_features))\n",
        "    if force_weights is None:\n",
        "        weights = np.zeros((n_clusters,))\n",
        "\n",
        "    for cc in range(n_clusters):\n",
        "        # update weights (alpha)\n",
        "        if force_weights is None:\n",
        "            weights[cc] = np.mean(posterior[cc, :])\n",
        "        else:\n",
        "            weights = force_weights\n",
        "\n",
        "        # update centers (mu)\n",
        "        X_scaled = X.copy()\n",
        "        if sp.issparse(X):\n",
        "            X_scaled.data *= posterior[cc, :].repeat(np.diff(X_scaled.indptr))\n",
        "        else:\n",
        "            for ee in range(n_examples):\n",
        "                X_scaled[ee, :] *= posterior[cc, ee]\n",
        "\n",
        "        centers[cc, :] = X_scaled.sum(axis=0)\n",
        "\n",
        "        # normalize centers\n",
        "        center_norm = np.linalg.norm(centers[cc, :])\n",
        "        if center_norm > 1e-8:\n",
        "            centers[cc, :] = centers[cc, :] / center_norm\n",
        "\n",
        "        # update concentration (kappa) [TODO: add other kappa approximations]\n",
        "        rbar = center_norm / (n_examples * weights[cc])\n",
        "        concentrations[cc] = rbar * n_features - np.power(rbar, 3.)\n",
        "        if np.abs(rbar - 1.0) < 1e-10:\n",
        "            concentrations[cc] = MAX_CONTENTRATION\n",
        "        else:\n",
        "            concentrations[cc] /= 1. - np.power(rbar, 2.)\n",
        "\n",
        "        # let python know we can free this (good for large dense X)\n",
        "        del X_scaled\n",
        "\n",
        "    return centers, weights, concentrations\n",
        "\n",
        "\n",
        "def _movMF(\n",
        "    X,\n",
        "    n_clusters,\n",
        "    posterior_type=\"soft\",\n",
        "    force_weights=None,\n",
        "    max_iter=300,\n",
        "    verbose=False,\n",
        "    init=\"random-class\",\n",
        "    random_state=None,\n",
        "    tol=1e-6,\n",
        "):\n",
        "    \"\"\"Mixture of von Mises Fisher clustering.\n",
        "\n",
        "    Implements the algorithms (i) and (ii) from\n",
        "\n",
        "      \"Clustering on the Unit Hypersphere using von Mises-Fisher Distributions\"\n",
        "      by Banerjee, Dhillon, Ghosh, and Sra.\n",
        "\n",
        "    TODO: Currently only supports Banerjee et al 2005 approximation of kappa,\n",
        "          however, there are numerous other approximations see _update_params.\n",
        "\n",
        "    Attribution\n",
        "    ----------\n",
        "    Approximation of log-vmf distribution function from movMF R-package.\n",
        "\n",
        "    movMF: An R Package for Fitting Mixtures of von Mises-Fisher Distributions\n",
        "    by Kurt Hornik, Bettina Grun, 2014\n",
        "\n",
        "    Find more at:\n",
        "      https://cran.r-project.org/web/packages/movMF/vignettes/movMF.pdf\n",
        "      https://cran.r-project.org/web/packages/movMF/index.html\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_clusters : int, optional, default: 8\n",
        "        The number of clusters to form as well as the number of\n",
        "        centroids to generate.\n",
        "\n",
        "    posterior_type: 'soft' or 'hard'\n",
        "        Type of posterior computed in exepectation step.\n",
        "        See note about attribute: self.posterior_\n",
        "\n",
        "    force_weights : None or array [n_clusters, ]\n",
        "        If None, the algorithm will estimate the weights.\n",
        "        If an array of weights, algorithm will estimate concentrations and\n",
        "        centers with given weights.\n",
        "\n",
        "    max_iter : int, default: 300\n",
        "        Maximum number of iterations of the k-means algorithm for a\n",
        "        single run.\n",
        "\n",
        "    n_init : int, default: 10\n",
        "        Number of time the k-means algorithm will be run with different\n",
        "        centroid seeds. The final results will be the best output of\n",
        "        n_init consecutive runs in terms of inertia.\n",
        "\n",
        "    init:  (string) one of\n",
        "        random-class [default]: random class assignment & centroid computation\n",
        "        k-means++ : uses sklearn k-means++ initialization algorithm\n",
        "        spherical-k-means : use centroids from one pass of spherical k-means\n",
        "        random : random unit norm vectors\n",
        "        random-orthonormal : random orthonormal vectors\n",
        "        If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
        "        and gives the initial centers.\n",
        "\n",
        "    tol : float, default: 1e-6\n",
        "        Relative tolerance with regards to inertia to declare convergence\n",
        "\n",
        "    n_jobs : int\n",
        "        The number of jobs to use for the computation. This works by computing\n",
        "        each of the n_init runs in parallel.\n",
        "        If -1 all CPUs are used. If 1 is given, no parallel computing code is\n",
        "        used at all, which is useful for debugging. For n_jobs below -1,\n",
        "        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one\n",
        "        are used.\n",
        "\n",
        "    random_state : integer or numpy.RandomState, optional\n",
        "        The generator used to initialize the centers. If an integer is\n",
        "        given, it fixes the seed. Defaults to the global numpy random\n",
        "        number generator.\n",
        "\n",
        "    verbose : int, default 0\n",
        "        Verbosity mode.\n",
        "\n",
        "    copy_x : boolean, default True\n",
        "        When pre-computing distances it is more numerically accurate to center\n",
        "        the data first.  If copy_x is True, then the original data is not\n",
        "        modified.  If False, the original data is modified, and put back before\n",
        "        the function returns, but small numerical differences may be introduced\n",
        "        by subtracting and then adding the data mean.\n",
        "    \"\"\"\n",
        "    random_state = check_random_state(random_state)\n",
        "    n_examples, n_features = np.shape(X)\n",
        "\n",
        "    # init centers (mus)\n",
        "    centers = _init_unit_centers(X, n_clusters, random_state, init)\n",
        "\n",
        "    # init weights (alphas)\n",
        "    if force_weights is None:\n",
        "        weights = np.ones((n_clusters,))\n",
        "        weights = weights / np.sum(weights)\n",
        "    else:\n",
        "        weights = force_weights\n",
        "\n",
        "    # init concentrations (kappas)\n",
        "    concentrations = np.ones((n_clusters,))\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Initialization complete\")\n",
        "\n",
        "    for iter in range(max_iter):\n",
        "        centers_prev = centers.copy()\n",
        "\n",
        "        # expectation step\n",
        "        posterior = _expectation(\n",
        "            X, centers, weights, concentrations, posterior_type=posterior_type\n",
        "        )\n",
        "\n",
        "        # maximization step\n",
        "        centers, weights, concentrations = _maximization(\n",
        "            X, posterior, force_weights=force_weights\n",
        "        )\n",
        "\n",
        "        # check convergence\n",
        "        tolcheck = squared_norm(centers_prev - centers)\n",
        "        if tolcheck <= tol:\n",
        "            if verbose:\n",
        "                print(\n",
        "                    \"Converged at iteration %d: \"\n",
        "                    \"center shift %e within tolerance %e\" % (iter, tolcheck, tol)\n",
        "                )\n",
        "            break\n",
        "\n",
        "    # labels come for free via posterior\n",
        "    labels = np.zeros((n_examples,))\n",
        "    for ee in range(n_examples):\n",
        "        labels[ee] = np.argmax(posterior[:, ee])\n",
        "\n",
        "    inertia = _inertia_from_labels(X, centers, labels)\n",
        "\n",
        "    return centers, weights, concentrations, posterior, labels, inertia\n",
        "\n",
        "\n",
        "def movMF(\n",
        "    X,\n",
        "    n_clusters,\n",
        "    posterior_type=\"soft\",\n",
        "    force_weights=None,\n",
        "    n_init=10,\n",
        "    n_jobs=1,\n",
        "    max_iter=300,\n",
        "    verbose=False,\n",
        "    init=\"random-class\",\n",
        "    random_state=None,\n",
        "    tol=1e-6,\n",
        "    copy_x=True,\n",
        "):\n",
        "    \"\"\"Wrapper for parallelization of _movMF and running n_init times.\n",
        "    \"\"\"\n",
        "    if n_init <= 0:\n",
        "        raise ValueError(\n",
        "            \"Invalid number of initializations.\"\n",
        "            \" n_init=%d must be bigger than zero.\" % n_init\n",
        "        )\n",
        "    random_state = check_random_state(random_state)\n",
        "\n",
        "    if max_iter <= 0:\n",
        "        raise ValueError(\n",
        "            \"Number of iterations should be a positive number,\"\n",
        "            \" got %d instead\" % max_iter\n",
        "        )\n",
        "\n",
        "    best_inertia = np.infty\n",
        "    X = as_float_array(X, copy=copy_x)\n",
        "    tol = _tolerance(X, tol)\n",
        "\n",
        "    if hasattr(init, \"__array__\"):\n",
        "        init = check_array(init, dtype=X.dtype.type, copy=True)\n",
        "        _validate_center_shape(X, n_clusters, init)\n",
        "\n",
        "        if n_init != 1:\n",
        "            warnings.warn(\n",
        "                \"Explicit initial center position passed: \"\n",
        "                \"performing only one init in k-means instead of n_init=%d\" % n_init,\n",
        "                RuntimeWarning,\n",
        "                stacklevel=2,\n",
        "            )\n",
        "            n_init = 1\n",
        "\n",
        "    # defaults\n",
        "    best_centers = None\n",
        "    best_labels = None\n",
        "    best_weights = None\n",
        "    best_concentrations = None\n",
        "    best_posterior = None\n",
        "    best_inertia = None\n",
        "\n",
        "    if n_jobs == 1:\n",
        "        # For a single thread, less memory is needed if we just store one set\n",
        "        # of the best results (as opposed to one set per run per thread).\n",
        "        for it in range(n_init):\n",
        "            # cluster on the sphere\n",
        "            (centers, weights, concentrations, posterior, labels, inertia) = _movMF(\n",
        "                X,\n",
        "                n_clusters,\n",
        "                posterior_type=posterior_type,\n",
        "                force_weights=force_weights,\n",
        "                max_iter=max_iter,\n",
        "                verbose=verbose,\n",
        "                init=init,\n",
        "                random_state=random_state,\n",
        "                tol=tol,\n",
        "            )\n",
        "\n",
        "            # determine if these results are the best so far\n",
        "            if best_inertia is None or inertia < best_inertia:\n",
        "                best_centers = centers.copy()\n",
        "                best_labels = labels.copy()\n",
        "                best_weights = weights.copy()\n",
        "                best_concentrations = concentrations.copy()\n",
        "                best_posterior = posterior.copy()\n",
        "                best_inertia = inertia\n",
        "    else:\n",
        "        # parallelisation of movMF runs\n",
        "        seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\n",
        "        results = Parallel(n_jobs=n_jobs, verbose=0)(\n",
        "            delayed(_movMF)(\n",
        "                X,\n",
        "                n_clusters,\n",
        "                posterior_type=posterior_type,\n",
        "                force_weights=force_weights,\n",
        "                max_iter=max_iter,\n",
        "                verbose=verbose,\n",
        "                init=init,\n",
        "                random_state=random_state,\n",
        "                tol=tol,\n",
        "            )\n",
        "            for seed in seeds\n",
        "        )\n",
        "\n",
        "        # Get results with the lowest inertia\n",
        "        centers, weights, concentrations, posteriors, labels, inertia = zip(*results)\n",
        "        best = np.argmin(inertia)\n",
        "        best_labels = labels[best]\n",
        "        best_inertia = inertia[best]\n",
        "        best_centers = centers[best]\n",
        "        best_concentrations = concentrations[best]\n",
        "        best_posterior = posteriors[best]\n",
        "        best_weights = weights[best]\n",
        "\n",
        "    return (\n",
        "        best_centers,\n",
        "        best_labels,\n",
        "        best_inertia,\n",
        "        best_weights,\n",
        "        best_concentrations,\n",
        "        best_posterior,\n",
        "    )\n",
        "\n",
        "\n",
        "class VonMisesFisherMixture(BaseEstimator, ClusterMixin, TransformerMixin):\n",
        "    \"\"\"Estimator for Mixture of von Mises Fisher clustering on the unit sphere.\n",
        "\n",
        "    Implements the algorithms (i) and (ii) from\n",
        "\n",
        "      \"Clustering on the Unit Hypersphere using von Mises-Fisher Distributions\"\n",
        "      by Banerjee, Dhillon, Ghosh, and Sra.\n",
        "\n",
        "    TODO: Currently only supports Banerjee et al 2005 approximation of kappa,\n",
        "          however, there are numerous other approximations see _update_params.\n",
        "\n",
        "    Attribution\n",
        "    ----------\n",
        "    Approximation of log-vmf distribution function from movMF R-package.\n",
        "\n",
        "    movMF: An R Package for Fitting Mixtures of von Mises-Fisher Distributions\n",
        "    by Kurt Hornik, Bettina Grun, 2014\n",
        "\n",
        "    Find more at:\n",
        "      https://cran.r-project.org/web/packages/movMF/vignettes/movMF.pdf\n",
        "      https://cran.r-project.org/web/packages/movMF/index.html\n",
        "\n",
        "    Basic sklearn scaffolding from sklearn.cluster.KMeans.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_clusters : int, optional, default: 8\n",
        "        The number of clusters to form as well as the number of\n",
        "        centroids to generate.\n",
        "\n",
        "    posterior_type: 'soft' or 'hard'\n",
        "        Type of posterior computed in exepectation step.\n",
        "        See note about attribute: self.posterior_\n",
        "\n",
        "    force_weights : None or array [n_clusters, ]\n",
        "        If None, the algorithm will estimate the weights.\n",
        "        If an array of weights, algorithm will estimate concentrations and\n",
        "        centers with given weights.\n",
        "\n",
        "    max_iter : int, default: 300\n",
        "        Maximum number of iterations of the k-means algorithm for a\n",
        "        single run.\n",
        "\n",
        "    n_init : int, default: 10\n",
        "        Number of time the k-means algorithm will be run with different\n",
        "        centroid seeds. The final results will be the best output of\n",
        "        n_init consecutive runs in terms of inertia.\n",
        "\n",
        "    init:  (string) one of\n",
        "        random-class [default]: random class assignment & centroid computation\n",
        "        k-means++ : uses sklearn k-means++ initialization algorithm\n",
        "        spherical-k-means : use centroids from one pass of spherical k-means\n",
        "        random : random unit norm vectors\n",
        "        random-orthonormal : random orthonormal vectors\n",
        "        If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
        "        and gives the initial centers.\n",
        "\n",
        "    tol : float, default: 1e-6\n",
        "        Relative tolerance with regards to inertia to declare convergence\n",
        "\n",
        "    n_jobs : int\n",
        "        The number of jobs to use for the computation. This works by computing\n",
        "        each of the n_init runs in parallel.\n",
        "        If -1 all CPUs are used. If 1 is given, no parallel computing code is\n",
        "        used at all, which is useful for debugging. For n_jobs below -1,\n",
        "        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one\n",
        "        are used.\n",
        "\n",
        "    random_state : integer or numpy.RandomState, optional\n",
        "        The generator used to initialize the centers. If an integer is\n",
        "        given, it fixes the seed. Defaults to the global numpy random\n",
        "        number generator.\n",
        "\n",
        "    verbose : int, default 0\n",
        "        Verbosity mode.\n",
        "\n",
        "    copy_x : boolean, default True\n",
        "        When pre-computing distances it is more numerically accurate to center\n",
        "        the data first.  If copy_x is True, then the original data is not\n",
        "        modified.  If False, the original data is modified, and put back before\n",
        "        the function returns, but small numerical differences may be introduced\n",
        "        by subtracting and then adding the data mean.\n",
        "\n",
        "    normalize : boolean, default True\n",
        "        Normalize the input to have unnit norm.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "\n",
        "    cluster_centers_ : array, [n_clusters, n_features]\n",
        "        Coordinates of cluster centers\n",
        "\n",
        "    labels_ :\n",
        "        Labels of each point\n",
        "\n",
        "    inertia_ : float\n",
        "        Sum of distances of samples to their closest cluster center.\n",
        "\n",
        "    weights_ : array, [n_clusters,]\n",
        "        Weights of each cluster in vMF distribution (alpha).\n",
        "\n",
        "    concentrations_ : array [n_clusters,]\n",
        "        Concentration parameter for each cluster (kappa).\n",
        "        Larger values correspond to more concentrated clusters.\n",
        "\n",
        "    posterior_ : array, [n_clusters, n_examples]\n",
        "        Each column corresponds to the posterio distribution for and example.\n",
        "\n",
        "        If posterior_type='hard' is used, there will only be one non-zero per\n",
        "        column, its index corresponding to the example's cluster label.\n",
        "\n",
        "        If posterior_type='soft' is used, this matrix will be dense and the\n",
        "        column values correspond to soft clustering weights.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_clusters=5,\n",
        "        posterior_type=\"soft\",\n",
        "        force_weights=None,\n",
        "        n_init=10,\n",
        "        n_jobs=1,\n",
        "        max_iter=300,\n",
        "        verbose=False,\n",
        "        init=\"random-class\",\n",
        "        random_state=None,\n",
        "        tol=1e-6,\n",
        "        copy_x=True,\n",
        "        normalize=True,\n",
        "    ):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.posterior_type = posterior_type\n",
        "        self.force_weights = force_weights\n",
        "        self.n_init = n_init\n",
        "        self.n_jobs = n_jobs\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "        self.init = init\n",
        "        self.random_state = random_state\n",
        "        self.tol = tol\n",
        "        self.copy_x = copy_x\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def _check_force_weights(self):\n",
        "        if self.force_weights is None:\n",
        "            return\n",
        "\n",
        "        if len(self.force_weights) != self.n_clusters:\n",
        "            raise ValueError(\n",
        "                (\n",
        "                    \"len(force_weights)={} but must equal \"\n",
        "                    \"n_clusters={}\".format(len(self.force_weights), self.n_clusters)\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def _check_fit_data(self, X):\n",
        "        \"\"\"Verify that the number of samples given is larger than k\"\"\"\n",
        "        X = check_array(X, accept_sparse=\"csr\", dtype=[np.float64, np.float32])\n",
        "        n_samples, n_features = X.shape\n",
        "        if X.shape[0] < self.n_clusters:\n",
        "            raise ValueError(\n",
        "                \"n_samples=%d should be >= n_clusters=%d\"\n",
        "                % (X.shape[0], self.n_clusters)\n",
        "            )\n",
        "\n",
        "        for ee in range(n_samples):\n",
        "            if sp.issparse(X):\n",
        "                n = sp.linalg.norm(X[ee, :])\n",
        "            else:\n",
        "                n = np.linalg.norm(X[ee, :])\n",
        "\n",
        "            if np.abs(n - 1.) > 1e-4:\n",
        "                raise ValueError(\"Data l2-norm must be 1, found {}\".format(n))\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _check_test_data(self, X):\n",
        "        X = check_array(X, accept_sparse=\"csr\", dtype=FLOAT_DTYPES, warn_on_dtype=True)\n",
        "        n_samples, n_features = X.shape\n",
        "        expected_n_features = self.cluster_centers_.shape[1]\n",
        "        if not n_features == expected_n_features:\n",
        "            raise ValueError(\n",
        "                \"Incorrect number of features. \"\n",
        "                \"Got %d features, expected %d\" % (n_features, expected_n_features)\n",
        "            )\n",
        "\n",
        "        for ee in range(n_samples):\n",
        "            if sp.issparse(X):\n",
        "                n = sp.linalg.norm(X[ee, :])\n",
        "            else:\n",
        "                n = np.linalg.norm(X[ee, :])\n",
        "\n",
        "            if np.abs(n - 1.) > 1e-4:\n",
        "                raise ValueError(\"Data l2-norm must be 1, found {}\".format(n))\n",
        "\n",
        "        return X\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Compute mixture of von Mises Fisher clustering.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
        "        \"\"\"\n",
        "        if self.normalize:\n",
        "            X = normalize(X)\n",
        "\n",
        "        self._check_force_weights()\n",
        "        random_state = check_random_state(self.random_state)\n",
        "        X = self._check_fit_data(X)\n",
        "\n",
        "        (\n",
        "            self.cluster_centers_,\n",
        "            self.labels_,\n",
        "            self.inertia_,\n",
        "            self.weights_,\n",
        "            self.concentrations_,\n",
        "            self.posterior_,\n",
        "        ) = movMF(\n",
        "            X,\n",
        "            self.n_clusters,\n",
        "            posterior_type=self.posterior_type,\n",
        "            force_weights=self.force_weights,\n",
        "            n_init=self.n_init,\n",
        "            n_jobs=self.n_jobs,\n",
        "            max_iter=self.max_iter,\n",
        "            verbose=self.verbose,\n",
        "            init=self.init,\n",
        "            random_state=random_state,\n",
        "            tol=self.tol,\n",
        "            copy_x=self.copy_x,\n",
        "        )\n",
        "\n",
        "        return self\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        \"\"\"Compute cluster centers and predict cluster index for each sample.\n",
        "        Convenience method; equivalent to calling fit(X) followed by\n",
        "        predict(X).\n",
        "        \"\"\"\n",
        "        return self.fit(X).labels_\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Compute clustering and transform X to cluster-distance space.\n",
        "        Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
        "        \"\"\"\n",
        "        # Currently, this just skips a copy of the data if it is not in\n",
        "        # np.array or CSR format already.\n",
        "        # XXX This skips _check_test_data, which may change the dtype;\n",
        "        # we should refactor the input validation.\n",
        "        return self.fit(X)._transform(X)\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Transform X to a cluster-distance space.\n",
        "        In the new space, each dimension is the cosine distance to the cluster\n",
        "        centers.  Note that even if X is sparse, the array returned by\n",
        "        `transform` will typically be dense.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
        "            New data to transform.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        X_new : array, shape [n_samples, k]\n",
        "            X transformed in the new space.\n",
        "        \"\"\"\n",
        "        if self.normalize:\n",
        "            X = normalize(X)\n",
        "\n",
        "        check_is_fitted(self, \"cluster_centers_\")\n",
        "        X = self._check_test_data(X)\n",
        "        return self._transform(X)\n",
        "\n",
        "    def _transform(self, X):\n",
        "        \"\"\"guts of transform method; no input validation\"\"\"\n",
        "        return cosine_distances(X, self.cluster_centers_)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the closest cluster each sample in X belongs to.\n",
        "        In the vector quantization literature, `cluster_centers_` is called\n",
        "        the code book and each value returned by `predict` is the index of\n",
        "        the closest code in the code book.\n",
        "\n",
        "        Note:  Does not check that each point is on the sphere.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
        "            New data to predict.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        labels : array, shape [n_samples,]\n",
        "            Index of the cluster each sample belongs to.\n",
        "        \"\"\"\n",
        "        if self.normalize:\n",
        "            X = normalize(X)\n",
        "\n",
        "        check_is_fitted(self, \"cluster_centers_\")\n",
        "\n",
        "        X = self._check_test_data(X)\n",
        "        return _labels_inertia(X, self.cluster_centers_)[0]\n",
        "\n",
        "    def score(self, X, y=None):\n",
        "        \"\"\"Inertia score (sum of all distances to closest cluster).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
        "            New data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        score : float\n",
        "            Larger score is better.\n",
        "        \"\"\"\n",
        "        if self.normalize:\n",
        "            X = normalize(X)\n",
        "\n",
        "        check_is_fitted(self, \"cluster_centers_\")\n",
        "        X = self._check_test_data(X)\n",
        "        return -_labels_inertia(X, self.cluster_centers_)[1]\n",
        "\n",
        "    def log_likelihood(self, X):\n",
        "        check_is_fitted(self, \"cluster_centers_\")\n",
        "\n",
        "        return _log_likelihood(\n",
        "            X, self.cluster_centers_, self.weights_, self.concentrations_\n",
        "        )\n"
      ],
      "metadata": {
        "id": "WPYL_EdEIRrz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from os.path import join\n",
        "from nltk import tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "# from transformers import pipeline\n",
        "# classifier = pipeline(\"zero-shot-classification\", model = \"facebook/bart-large-mnli\")\n",
        "def read_file(data_dir, with_evaluation):\n",
        "    data = []\n",
        "    target = []\n",
        "    labels_used = [\"bad\",\"good\"]\n",
        "    with open(join(data_dir, 'combined_file_latest.csv'), 'rt', encoding='utf-8') as csvfile:\n",
        "        csv.field_size_limit(500 * 1024 * 1024)\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            if data_dir == './News' :\n",
        "                doc = row[1] + '. ' + row[2]\n",
        "                data.append(doc)\n",
        "                #removed -1\n",
        "                target.append(int(row[1]))\n",
        "            elif data_dir == './Movies':\n",
        "                data.append(row[1])\n",
        "                #removede -1\n",
        "                target.append(int(row[0]))\n",
        "                #target.append(1 if classifier(row[1],labels_used)[\"labels\"][0] == \"good\" else 0)\n",
        "    if with_evaluation:\n",
        "        y = np.asarray(target)\n",
        "        #print(y)\n",
        "        y[y==-1]=0\n",
        "        assert len(data) == len(y)\n",
        "        assert set(range(len(np.unique(y)))) == set(np.unique(y))\n",
        "    else:\n",
        "        y = None\n",
        "    return data, y\n",
        "\n",
        "\n",
        "def clean_str(string):\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),.!?_\\\"\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\\"\", \" \\\" \", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'m\", \" \\'m\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"\\.\", \" . \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\$\", \" $ \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "\n",
        "def preprocess_doc(data):\n",
        "    data = [s.strip() for s in data]\n",
        "    data = [clean_str(s) for s in data]\n",
        "    return data\n",
        "\n",
        "\n",
        "def pad_sequences(sentences, padding_word=\"<PAD/>\", pad_len=None):\n",
        "    if pad_len is not None:\n",
        "        sequence_length = pad_len\n",
        "    else:\n",
        "        sequence_length = max(len(x) for x in sentences)\n",
        "\n",
        "    padded_sentences = []\n",
        "    for i in range(len(sentences)):\n",
        "        sentence = sentences[i]\n",
        "        num_padding = sequence_length - len(sentence)\n",
        "        new_sentence = sentence + [padding_word] * num_padding\n",
        "        padded_sentences.append(new_sentence)\n",
        "    return padded_sentences\n",
        "\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    # Build vocabulary\n",
        "    word_counts = Counter(itertools.chain(*sentences))\n",
        "    # Mapping from index to word\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "    # Mapping from word to index\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return word_counts, vocabulary, vocabulary_inv\n",
        "\n",
        "\n",
        "def build_input_data_cnn(sentences, vocabulary):\n",
        "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_input_data_rnn(data, vocabulary, max_doc_len, max_sent_len):\n",
        "    x = np.zeros((len(data), max_doc_len, max_sent_len), dtype='int32')\n",
        "    for i, doc in enumerate(data):\n",
        "        for j, sent in enumerate(doc):\n",
        "            k = 0\n",
        "            for word in sent:\n",
        "                x[i,j,k] = vocabulary[word]\n",
        "                k += 1\n",
        "    return x\n",
        "\n",
        "\n",
        "def extract_keywords(data_path, vocab, class_type, num_keywords, data, perm):\n",
        "    sup_data = []\n",
        "    sup_idx = []\n",
        "    sup_label = []\n",
        "    file_name = 'doc_id.txt'\n",
        "    infile = open(join(data_path, file_name), mode='r', encoding='utf-8')\n",
        "    text = infile.readlines()\n",
        "    for i, line in enumerate(text):\n",
        "        line = line.split('\\n')[0]\n",
        "        class_id, doc_ids = line.split(':')\n",
        "        assert int(class_id) == i\n",
        "        seed_idx = doc_ids.split(',')\n",
        "        seed_idx = [int(idx) for idx in seed_idx]\n",
        "        sup_idx.append(seed_idx)\n",
        "        for idx in seed_idx:\n",
        "            sup_data.append(\" \".join(data[idx]))\n",
        "            sup_label.append(i)\n",
        "\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    import nltk\n",
        "\n",
        "    tfidf = TfidfVectorizer(norm='l2', sublinear_tf=True, max_df=0.2, stop_words='english')\n",
        "    sup_x = tfidf.fit_transform(sup_data)\n",
        "    sup_x = np.asarray(sup_x.todense())\n",
        "\n",
        "    vocab_dict = tfidf.vocabulary_\n",
        "    vocab_inv_dict = {v: k for k, v in vocab_dict.items()}\n",
        "\n",
        "    print(\"\\n### Supervision type: Labeled documents ###\")\n",
        "    print(\"Extracted keywords for each class: \")\n",
        "    keywords = []\n",
        "    cnt = 0\n",
        "    for i in range(len(sup_idx)):\n",
        "        class_vec = np.average(sup_x[cnt:cnt+len(sup_idx[i])], axis=0)\n",
        "        cnt += len(sup_idx[i])\n",
        "        sort_idx = np.argsort(class_vec)[::-1]\n",
        "        keyword = []\n",
        "        if class_type == 'topic':\n",
        "            j = 0\n",
        "            k = 0\n",
        "            while j < num_keywords:\n",
        "                w = vocab_inv_dict[sort_idx[k]]\n",
        "                if w in vocab:\n",
        "                    keyword.append(vocab_inv_dict[sort_idx[k]])\n",
        "                    j += 1\n",
        "                k += 1\n",
        "        elif class_type == 'sentiment':\n",
        "            j = 0\n",
        "            k = 0\n",
        "            while j < num_keywords:\n",
        "                w = vocab_inv_dict[sort_idx[k]]\n",
        "                w, t = nltk.pos_tag([w])[0]\n",
        "                if t.startswith(\"J\") and w in vocab:\n",
        "                    keyword.append(w)\n",
        "                    j += 1\n",
        "                k += 1\n",
        "        print(\"Class {}:\".format(i))\n",
        "        print(keyword)\n",
        "        keywords.append(keyword)\n",
        "\n",
        "    new_sup_idx = []\n",
        "    m = {v: k for k, v in enumerate(perm)}\n",
        "    for seed_idx in sup_idx:\n",
        "        new_seed_idx = []\n",
        "        for ele in seed_idx:\n",
        "            new_seed_idx.append(m[ele])\n",
        "        new_sup_idx.append(new_seed_idx)\n",
        "    new_sup_idx = np.asarray(new_sup_idx)\n",
        "\n",
        "    return keywords, new_sup_idx\n",
        "\n",
        "\n",
        "def load_keywords(data_path, sup_source):\n",
        "    if sup_source == 'labels':\n",
        "        file_name = 'classes.txt'\n",
        "        print(\"\\n### Supervision type: Label Surface Names ###\")\n",
        "        print(\"Label Names for each class: \")\n",
        "    elif sup_source == 'keywords':\n",
        "        file_name = 'keywords.txt'\n",
        "        print(\"\\n### Supervision type: Class-related Keywords ###\")\n",
        "        print(\"Keywords for each class: \")\n",
        "    infile = open(join(data_path, file_name), mode='r', encoding='utf-8')\n",
        "    text = infile.readlines()\n",
        "\n",
        "    keywords = []\n",
        "    for i, line in enumerate(text):\n",
        "        line = line.split('\\n')[0]\n",
        "        class_id, contents = line.split(':')\n",
        "        assert int(class_id) == i\n",
        "        keyword = contents.split(',')\n",
        "        print(\"Supervision content of class {}:\".format(i))\n",
        "        print(keyword)\n",
        "        keywords.append(keyword)\n",
        "    return keywords\n",
        "\n",
        "\n",
        "def load_cnn(dataset_name, sup_source, num_keywords=10, with_evaluation=True, truncate_len=None):\n",
        "    data_path = './' + dataset_name\n",
        "    data, y = read_file(data_path, with_evaluation)\n",
        "\n",
        "    sz = len(data)\n",
        "    np.random.seed(1234)\n",
        "    perm = np.random.permutation(sz)\n",
        "\n",
        "    data = preprocess_doc(data)\n",
        "    data = [s.split(\" \") for s in data]\n",
        "\n",
        "    tmp_list = [len(doc) for doc in data]\n",
        "    len_max = max(tmp_list)\n",
        "    len_avg = np.average(tmp_list)\n",
        "    len_std = np.std(tmp_list)\n",
        "\n",
        "    print(\"\\n### Dataset statistics: ###\")\n",
        "    print('Document max length: {} (words)'.format(len_max))\n",
        "    print('Document average length: {} (words)'.format(len_avg))\n",
        "    print('Document length std: {} (words)'.format(len_std))\n",
        "\n",
        "    if truncate_len is None:\n",
        "        truncate_len = min(int(len_avg + 3*len_std), len_max)\n",
        "    print(\"Defined maximum document length: {} (words)\".format(truncate_len))\n",
        "    print('Fraction of truncated documents: {}'.format(sum(tmp > truncate_len for tmp in tmp_list)/len(tmp_list)))\n",
        "\n",
        "    sequences_padded = pad_sequences(data)\n",
        "    word_counts, vocabulary, vocabulary_inv = build_vocab(sequences_padded)\n",
        "    x = build_input_data_cnn(sequences_padded, vocabulary)\n",
        "    x = x[perm]\n",
        "\n",
        "    if with_evaluation:\n",
        "        print(\"Number of classes: {}\".format(len(np.unique(y))))\n",
        "        print(\"Number of documents in each class:\")\n",
        "        for i in range(len(np.unique(y))):\n",
        "            print(\"Class {}: {}\".format(i, len(np.where(y == i)[0])))\n",
        "        y = y[perm]\n",
        "\n",
        "    print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))\n",
        "\n",
        "    if sup_source == 'labels' or sup_source == 'keywords':\n",
        "        keywords = load_keywords(data_path, sup_source)\n",
        "        return x, y, word_counts, vocabulary, vocabulary_inv, len_avg, len_std, keywords, perm\n",
        "    elif sup_source == 'docs':\n",
        "        if dataset_name == 'nyt':\n",
        "            class_type = 'topic'\n",
        "        elif dataset_name == 'News':\n",
        "            class_type = 'topic'\n",
        "        elif dataset_name == 'Movies':\n",
        "            class_type = 'sentiment'\n",
        "        keywords, sup_idx = extract_keywords(data_path, vocabulary, class_type, num_keywords, data, perm)\n",
        "        return x, y, word_counts, vocabulary, vocabulary_inv, len_avg, len_std, keywords, sup_idx, perm\n",
        "\n",
        "\n",
        "def load_rnn(dataset_name, sup_source, num_keywords=10, with_evaluation=True, truncate_len=None):\n",
        "    data_path = './' + dataset_name\n",
        "    data, y = read_file(data_path, with_evaluation)\n",
        "\n",
        "    sz = len(data)\n",
        "    np.random.seed(1234)\n",
        "    perm = np.random.permutation(sz)\n",
        "\n",
        "    data = preprocess_doc(data)\n",
        "    data_copy = [s.split(\" \") for s in data]\n",
        "    docs_padded = pad_sequences(data_copy)\n",
        "    word_counts, vocabulary, vocabulary_inv = build_vocab(docs_padded)\n",
        "\n",
        "    data = [tokenize.sent_tokenize(doc) for doc in data]\n",
        "    flat_data = [sent for doc in data for sent in doc]\n",
        "\n",
        "    tmp_list = [len(sent.split(\" \")) for sent in flat_data]\n",
        "    max_sent_len = max(tmp_list)\n",
        "    avg_sent_len = np.average(tmp_list)\n",
        "    std_sent_len = np.std(tmp_list)\n",
        "\n",
        "    print(\"\\n### Dataset statistics: ###\")\n",
        "    print('Sentence max length: {} (words)'.format(max_sent_len))\n",
        "    print('Sentence average length: {} (words)'.format(avg_sent_len))\n",
        "\n",
        "    if truncate_len is None:\n",
        "        truncate_sent_len = min(int(avg_sent_len + 3*std_sent_len), max_sent_len)\n",
        "    else:\n",
        "        truncate_sent_len = truncate_len[1]\n",
        "    print(\"Defined maximum sentence length: {} (words)\".format(truncate_sent_len))\n",
        "    print('Fraction of truncated sentences: {}'.format(sum(tmp > truncate_sent_len for tmp in tmp_list)/len(tmp_list)))\n",
        "\n",
        "    tmp_list = [len(doc) for doc in data]\n",
        "    max_doc_len = max(tmp_list)\n",
        "    avg_doc_len = np.average(tmp_list)\n",
        "    std_doc_len = np.std(tmp_list)\n",
        "\n",
        "    print('Document max length: {} (sentences)'.format(max_doc_len))\n",
        "    print('Document average length: {} (sentences)'.format(avg_doc_len))\n",
        "\n",
        "    if truncate_len is None:\n",
        "        truncate_doc_len = min(int(avg_doc_len + 3*std_doc_len), max_doc_len)\n",
        "    else:\n",
        "        truncate_doc_len = truncate_len[0]\n",
        "    print(\"Defined maximum document length: {} (sentences)\".format(truncate_doc_len))\n",
        "    print('Fraction of truncated documents: {}'.format(sum(tmp > truncate_doc_len for tmp in tmp_list)/len(tmp_list)))\n",
        "\n",
        "    len_avg = [avg_doc_len, avg_sent_len]\n",
        "    len_std = [std_doc_len, std_sent_len]\n",
        "\n",
        "    data = [[sent.split(\" \") for sent in doc] for doc in data]\n",
        "    x = build_input_data_rnn(data, vocabulary, max_doc_len, max_sent_len)\n",
        "    x = x[perm]\n",
        "\n",
        "    if with_evaluation:\n",
        "        print(\"Number of classes: {}\".format(len(np.unique(y))))\n",
        "        print(\"Number of documents in each class:\")\n",
        "        for i in range(len(np.unique(y))):\n",
        "            print(\"Class {}: {}\".format(i, len(np.where(y == i)[0])))\n",
        "        y = y[perm]\n",
        "\n",
        "    print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))\n",
        "\n",
        "    if sup_source == 'labels' or sup_source == 'keywords':\n",
        "        keywords = load_keywords(data_path, sup_source)\n",
        "        return x, y, word_counts, vocabulary, vocabulary_inv, len_avg, len_std, keywords, perm\n",
        "    elif sup_source == 'docs':\n",
        "        if dataset_name == 'nyt':\n",
        "            class_type = 'topic'\n",
        "        elif dataset_name == 'News':\n",
        "            class_type = 'topic'\n",
        "        elif dataset_name == 'Movies':\n",
        "            class_type = 'sentiment'\n",
        "        keywords, sup_idx = extract_keywords(data_path, vocabulary, class_type, num_keywords, data_copy, perm)\n",
        "        return x, y, word_counts, vocabulary, vocabulary_inv, len_avg, len_std, keywords, sup_idx, perm\n",
        "\n",
        "\n",
        "def load_dataset(dataset_name, sup_source, model='cnn', with_evaluation=True, truncate_len=None):\n",
        "    if model == 'cnn':\n",
        "        return load_cnn(dataset_name, sup_source, with_evaluation=with_evaluation, truncate_len=truncate_len)\n",
        "    elif model == 'rnn':\n",
        "        return load_rnn(dataset_name, sup_source, with_evaluation=with_evaluation, truncate_len=truncate_len)\n"
      ],
      "metadata": {
        "id": "pSjW5pMtIwFx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "np.random.seed(1234)\n",
        "from sklearn.cluster import KMeans\n",
        "from spherecluster import SphericalKMeans\n",
        "from spherecluster import *\n",
        "def seed_expansion(word_sup_array, prob_sup_array, sz, write_path, vocabulary_inv, embedding_mat):\n",
        "    expanded_seed = []\n",
        "    vocab_sz = len(vocabulary_inv)\n",
        "    for j, word_class in enumerate(word_sup_array):\n",
        "        prob_sup_class = prob_sup_array[j]\n",
        "        expanded_class = []\n",
        "        seed_vec = np.zeros(vocab_sz)\n",
        "        if len(word_class) < sz:\n",
        "            for i, word in enumerate(word_class):\n",
        "                seed_vec[word] = prob_sup_class[i]\n",
        "            expanded = np.dot(embedding_mat.transpose(), seed_vec)\n",
        "            expanded = np.dot(embedding_mat, expanded)\n",
        "            word_expanded = sorted(range(len(expanded)), key=lambda k: expanded[k], reverse=True)\n",
        "            for i in range(sz):\n",
        "                expanded_class.append(word_expanded[i])\n",
        "            expanded_seed.append(np.array(expanded_class))\n",
        "        else:\n",
        "            expanded_seed.append(word_class)\n",
        "        if write_path is not None:\n",
        "            if not os.path.exists(write_path):\n",
        "                os.makedirs(write_path)\n",
        "            f = open(write_path + 'class' + str(j) + '_' + str(sz) + '.txt', 'w')\n",
        "            for i, word in enumerate(expanded_class):\n",
        "                f.write(vocabulary_inv[word] + ' ')\n",
        "            f.close()\n",
        "    return expanded_seed\n",
        "\n",
        "\n",
        "def label_expansion(class_labels, write_path, vocabulary_inv, embedding_mat):\n",
        "    print(\"Retrieving top-t nearest words...\")\n",
        "    n_classes = len(class_labels)\n",
        "    prob_sup_array = []\n",
        "    current_szes = []\n",
        "    all_class_labels = []\n",
        "    for class_label in class_labels:\n",
        "        current_sz = len(class_label)\n",
        "        current_szes.append(current_sz)\n",
        "        prob_sup_array.append([1/current_sz] * current_sz)\n",
        "        all_class_labels += list(class_label)\n",
        "    current_sz = np.min(current_szes)\n",
        "    while len(all_class_labels) == len(set(all_class_labels)):\n",
        "        current_sz += 1\n",
        "        expanded_array = seed_expansion(class_labels, prob_sup_array, current_sz, None, vocabulary_inv, embedding_mat)\n",
        "        all_class_labels = [w for w_class in expanded_array for w in w_class]\n",
        "\n",
        "    expanded_array = seed_expansion(class_labels, prob_sup_array, current_sz-1, None, vocabulary_inv, embedding_mat)\n",
        "    print(\"Final expansion size t = {}\".format(len(expanded_array[0])))\n",
        "\n",
        "    centers = []\n",
        "    kappas = []\n",
        "    print(\"Top-t nearest words for each class:\")\n",
        "    for i in range(n_classes):\n",
        "        expanded_class = expanded_array[i]\n",
        "        vocab_expanded = [vocabulary_inv[w] for w in expanded_class]\n",
        "        print(\"Class {}:\".format(i))\n",
        "        print(vocab_expanded)\n",
        "        expanded_mat = embedding_mat[np.asarray(expanded_class)]\n",
        "        vmf_soft = VonMisesFisherMixture(n_clusters=1, n_jobs=15)\n",
        "        vmf_soft.fit(expanded_mat)\n",
        "        center = vmf_soft.cluster_centers_[0]\n",
        "        kappa = vmf_soft.concentrations_[0]\n",
        "        centers.append(center)\n",
        "        kappas.append(kappa)\n",
        "\n",
        "    for j, expanded_class in enumerate(expanded_array):\n",
        "        if write_path is not None:\n",
        "            if not os.path.exists(write_path):\n",
        "                os.makedirs(write_path)\n",
        "            f = open(write_path + 'class' + str(j) + '.txt', 'w')\n",
        "            for i, word in enumerate(expanded_class):\n",
        "                f.write(vocabulary_inv[word] + ' ')\n",
        "            f.close()\n",
        "    print(\"Finished vMF distribution fitting.\")\n",
        "    return expanded_array, centers, kappas\n",
        "\n",
        "\n",
        "def pseudodocs(word_sup_array, total_num, background_array, sequence_length, len_avg,\n",
        "                len_std, num_doc, interp_weight, vocabulary_inv, embedding_mat, model, save_dir=None):\n",
        "\n",
        "    for i in range(len(embedding_mat)):\n",
        "        embedding_mat[i] = embedding_mat[i] / np.linalg.norm(embedding_mat[i])\n",
        "\n",
        "    _, centers, kappas = \\\n",
        "    label_expansion(word_sup_array, save_dir, vocabulary_inv, embedding_mat)\n",
        "    print(word_sup_array)\n",
        "    print(\"Pseudo documents generation...\")\n",
        "    background_vec = interp_weight * background_array\n",
        "    if model == 'cnn':\n",
        "        docs = np.zeros((num_doc*len(word_sup_array), sequence_length), dtype='int32')\n",
        "        label = np.zeros((num_doc*len(word_sup_array), len(word_sup_array)))\n",
        "        for i in range(len(word_sup_array)):\n",
        "            docs_len = len_avg*np.ones(num_doc)\n",
        "            center = centers[i]\n",
        "            kappa = kappas[i]\n",
        "            discourses = sample_vMF(center, kappa, num_doc)\n",
        "            for j in range(num_doc):\n",
        "                discourse = discourses[j]\n",
        "                prob_vec = np.dot(embedding_mat, discourse)\n",
        "                prob_vec = np.exp(prob_vec)\n",
        "                sorted_idx = np.argsort(prob_vec)[::-1]\n",
        "                delete_idx = sorted_idx[total_num:]\n",
        "                prob_vec[delete_idx] = 0\n",
        "                prob_vec /= np.sum(prob_vec)\n",
        "                prob_vec *= 1 - interp_weight\n",
        "                prob_vec += background_vec\n",
        "                doc_len = int(docs_len[j])\n",
        "                docs[i*num_doc+j][:doc_len] = np.random.choice(len(prob_vec), size=doc_len, p=prob_vec)\n",
        "                label[i*num_doc+j] = interp_weight/len(word_sup_array)*np.ones(len(word_sup_array))\n",
        "                label[i*num_doc+j][i] += 1 - interp_weight\n",
        "    elif model == 'rnn':\n",
        "        docs = np.zeros((num_doc*len(word_sup_array), sequence_length[0], sequence_length[1]), dtype='int32')\n",
        "        label = np.zeros((num_doc*len(word_sup_array), len(word_sup_array)))\n",
        "        doc_len = int(len_avg[0])\n",
        "        sent_len = int(len_avg[1])\n",
        "        for period_idx in vocabulary_inv:\n",
        "            if vocabulary_inv[period_idx] == '.':\n",
        "                break\n",
        "        for i in range(len(word_sup_array)):\n",
        "            center = centers[i]\n",
        "            kappa = kappas[i]\n",
        "            discourses = sample_vMF(center, kappa, num_doc)\n",
        "            for j in range(num_doc):\n",
        "                discourse = discourses[j]\n",
        "                prob_vec = np.dot(embedding_mat, discourse)\n",
        "                prob_vec = np.exp(prob_vec)\n",
        "                sorted_idx = np.argsort(prob_vec)[::-1]\n",
        "                delete_idx = sorted_idx[total_num:]\n",
        "                prob_vec[delete_idx] = 0\n",
        "                prob_vec /= np.sum(prob_vec)\n",
        "                prob_vec *= 1 - interp_weight\n",
        "                prob_vec += background_vec\n",
        "                for k in range(doc_len):\n",
        "                    docs[i*num_doc+j][k][:sent_len] = np.random.choice(len(prob_vec), size=sent_len, p=prob_vec)\n",
        "                    docs[i*num_doc+j][k][sent_len] = period_idx\n",
        "                label[i*num_doc+j] = interp_weight/len(word_sup_array)*np.ones(len(word_sup_array))\n",
        "                label[i*num_doc+j][i] += 1 - interp_weight\n",
        "\n",
        "    print(\"Finished Pseudo documents generation.\")\n",
        "    print(\"docs\",docs)\n",
        "    print(\"label\",label)\n",
        "    return docs, label\n",
        "\n",
        "\n",
        "def augment(x, sup_idx, total_len):\n",
        "    print(\"Labeled documents augmentation...\")\n",
        "    docs = x[sup_idx.flatten()]\n",
        "    curr_len = len(docs)\n",
        "    copy_times = int(total_len/curr_len) - 1\n",
        "    y = np.zeros(len(sup_idx.flatten()), dtype='int32')\n",
        "    label_nums = [len(seed_idx) for seed_idx in sup_idx]\n",
        "    cnt = 0\n",
        "    for i in range(len(sup_idx)):\n",
        "        y[cnt:cnt+label_nums[i]] = i\n",
        "        cnt += label_nums[i]\n",
        "\n",
        "    new_docs = docs\n",
        "    new_y = y\n",
        "    for i in range(copy_times):\n",
        "        new_docs = np.concatenate((new_docs, docs), axis=0)\n",
        "        new_y = np.concatenate((new_y, y), axis=0)\n",
        "\n",
        "    pretrain_labels = np.zeros((len(new_y),len(np.unique(y))))\n",
        "    for i in range(len(new_y)):\n",
        "        pretrain_labels[i][new_y[i]] = 1.0\n",
        "\n",
        "    print(\"Finished labeled documents augmentation.\")\n",
        "    return new_docs, pretrain_labels\n"
      ],
      "metadata": {
        "id": "mmBcTIP4I1_U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(1234)\n",
        "import os\n",
        "from time import time\n",
        "import csv\n",
        "import keras.backend as K\n",
        "# K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=30, inter_op_parallelism_threads=30)))\n",
        "from keras.layers import Layer\n",
        "from keras.layers import Dense, Input, Convolution1D, Embedding, GlobalMaxPooling1D, GRU, TimeDistributed\n",
        "from keras.layers import Concatenate\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints\n",
        "from keras.initializers import VarianceScaling, RandomUniform\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.int64)\n",
        "    assert y_pred.size == y_true.size\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
        "    return f1_macro, f1_micro\n",
        "\n",
        "\n",
        "def ConvolutionLayer(input_shape, n_classes, filter_sizes=[2, 3, 4, 5], num_filters=20, word_trainable=False, vocab_sz=None,\n",
        "                     embedding_matrix=None, word_embedding_dim=100, hidden_dim=20, act='relu', init='ones'):\n",
        "    x = Input(shape=(input_shape,), name='input')\n",
        "    z = Embedding(vocab_sz, word_embedding_dim, input_length=(input_shape,), name=\"embedding\",\n",
        "                    weights=[embedding_matrix], trainable=word_trainable)(x)\n",
        "\n",
        "    conv_blocks = []\n",
        "    for sz in filter_sizes:\n",
        "        conv = Convolution1D(filters=num_filters,\n",
        "                             kernel_size=sz,\n",
        "                             padding=\"valid\",\n",
        "                             activation=act,\n",
        "                             strides=1,\n",
        "                             kernel_initializer=init)(z)\n",
        "        conv = GlobalMaxPooling1D()(conv)\n",
        "        conv_blocks.append(conv)\n",
        "    z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
        "    z = Dense(hidden_dim, activation=\"relu\")(z)\n",
        "    y = Dense(n_classes, activation=\"softmax\")(z)\n",
        "    return Model(inputs=x, outputs=y, name='classifier')\n",
        "\n",
        "\n",
        "def dot_product(x, kernel):\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "\n",
        "\n",
        "class AttentionWithContext(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 init='glorot_uniform', bias=True, **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = init\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        uit = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            uit += self.b\n",
        "\n",
        "        uit = K.tanh(uit)\n",
        "        ait = dot_product(uit, self.u)\n",
        "\n",
        "        a = K.exp(ait)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n",
        "\n",
        "\n",
        "def HierAttLayer(input_shape, n_classes, word_trainable=False, vocab_sz=None,\n",
        "                embedding_matrix=None, word_embedding_dim=100, gru_dim=100, fc_dim=100):\n",
        "    sentence_input = Input(shape=(input_shape[2],), dtype='int32')\n",
        "    embedded_sequences = Embedding(vocab_sz,\n",
        "                                    word_embedding_dim,\n",
        "                                    input_length=input_shape[2],\n",
        "                                    weights=[embedding_matrix],\n",
        "                                    trainable=word_trainable)(sentence_input)\n",
        "    l_lstm = GRU(gru_dim, return_sequences=True)(embedded_sequences)\n",
        "    l_dense = TimeDistributed(Dense(fc_dim))(l_lstm)\n",
        "    l_att = AttentionWithContext()(l_dense)\n",
        "    sentEncoder = Model(sentence_input, l_att)\n",
        "\n",
        "    x = Input(shape=(input_shape[1], input_shape[2]), dtype='int32')\n",
        "    review_encoder = TimeDistributed(sentEncoder)(x)\n",
        "    l_lstm_sent = GRU(gru_dim, return_sequences=True)(review_encoder)\n",
        "    l_dense_sent = TimeDistributed(Dense(fc_dim))(l_lstm_sent)\n",
        "    l_att_sent = AttentionWithContext()(l_dense_sent)\n",
        "    y = Dense(n_classes, activation='softmax')(l_att_sent)\n",
        "\n",
        "    return Model(inputs=x, outputs=y, name='classifier')\n",
        "\n",
        "\n",
        "class WSTC(object):\n",
        "    def __init__(self,\n",
        "                 input_shape,\n",
        "                 n_classes=None,\n",
        "                 init=RandomUniform(minval=-0.01, maxval=0.01),\n",
        "                 y=None,\n",
        "                 model='cnn',\n",
        "                 vocab_sz=None,\n",
        "                 word_embedding_dim=100,\n",
        "                 embedding_matrix=None\n",
        "                 ):\n",
        "\n",
        "        super(WSTC, self).__init__()\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "        self.y = y\n",
        "        self.n_classes = n_classes\n",
        "        if model == 'cnn':\n",
        "            self.classifier = ConvolutionLayer(self.input_shape[1], n_classes=n_classes,\n",
        "                                                vocab_sz=vocab_sz, embedding_matrix=embedding_matrix,\n",
        "                                                word_embedding_dim=word_embedding_dim, init=init)\n",
        "        elif model == 'rnn':\n",
        "            self.classifier = HierAttLayer(self.input_shape, n_classes=n_classes,\n",
        "                                             vocab_sz=vocab_sz, embedding_matrix=embedding_matrix,\n",
        "                                             word_embedding_dim=word_embedding_dim)\n",
        "\n",
        "        self.model = self.classifier\n",
        "        self.sup_list = {}\n",
        "\n",
        "    def pretrain(self, x, pretrain_labels, sup_idx=None, optimizer='adam',\n",
        "                 loss='kld', epochs=200, batch_size=256, save_dir=None):\n",
        "\n",
        "        self.classifier.compile(optimizer=optimizer, loss=loss)\n",
        "        print(\"\\nNeural model summary: \")\n",
        "        self.model.summary()\n",
        "\n",
        "        if sup_idx is not None:\n",
        "            for i, seed_idx in enumerate(sup_idx):\n",
        "                for idx in seed_idx:\n",
        "                    self.sup_list[idx] = i\n",
        "\n",
        "        # begin pretraining\n",
        "        t0 = time()\n",
        "        print('\\nPretraining...')\n",
        "        self.classifier.fit(x, pretrain_labels, batch_size=batch_size, epochs=epochs)\n",
        "        print('Pretraining time: {:.2f}s'.format(time() - t0))\n",
        "        if save_dir is not None:\n",
        "            if not os.path.exists(save_dir):\n",
        "                os.makedirs(save_dir)\n",
        "            self.classifier.save_weights(save_dir + '/pretrained.h5')\n",
        "            print('Pretrained model saved to {}/pretrained.h5'.format(save_dir))\n",
        "        self.pretrained = True\n",
        "\n",
        "    def load_weights(self, weights):\n",
        "        self.model.load_weights(weights)\n",
        "\n",
        "    def predict(self, x):\n",
        "        q = self.model.predict(x, verbose=0)\n",
        "        return q.argmax(1)\n",
        "\n",
        "    def target_distribution(self, q, power=2):\n",
        "        weight = q**power / q.sum(axis=0)\n",
        "        p = (weight.T / weight.sum(axis=1)).T\n",
        "        for i in self.sup_list:\n",
        "            p[i] = 0\n",
        "            p[i][self.sup_list[i]] = 1\n",
        "        return p\n",
        "\n",
        "    def compile(self, optimizer='sgd', loss='kld'):\n",
        "        self.model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    def fit(self, x, y=None, maxiter=5e4, batch_size=256, tol=0.1, power=2,\n",
        "            update_interval=140, save_dir=None, save_suffix=''):\n",
        "\n",
        "        print('Update interval: {}'.format(update_interval))\n",
        "\n",
        "        pred = self.classifier.predict(x)\n",
        "        y_pred = np.argmax(pred, axis=1)\n",
        "        y_pred_last = np.copy(y_pred)\n",
        "\n",
        "        # logging file\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        logfile = open(save_dir + '/self_training_log_{}.csv'.format(save_suffix), 'w')\n",
        "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'f1_macro', 'f1_micro'])\n",
        "        logwriter.writeheader()\n",
        "\n",
        "        index = 0\n",
        "        index_array = np.arange(x.shape[0])\n",
        "        for ite in range(int(maxiter)):\n",
        "            if ite % update_interval == 0:\n",
        "                q = self.model.predict(x, verbose=0)\n",
        "\n",
        "                y_pred = q.argmax(axis=1)\n",
        "                p = self.target_distribution(q, power)\n",
        "                print('\\nIter {}: '.format(ite), end='')\n",
        "                if y is not None:\n",
        "                    f1_macro, f1_micro = np.round(f1(y, y_pred), 5)\n",
        "                    logdict = dict(iter=ite, f1_macro=f1_macro, f1_micro=f1_micro)\n",
        "                    logwriter.writerow(logdict)\n",
        "                    print('f1_macro = {}, f1_micro = {}'.format(f1_macro, f1_micro))\n",
        "\n",
        "                # check stop criterion\n",
        "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float) / y_pred.shape[0]\n",
        "                y_pred_last = np.copy(y_pred)\n",
        "                print('Fraction of documents with label changes: {} %'.format(np.round(delta_label*100, 3)))\n",
        "                if ite > 0 and delta_label < tol/100:\n",
        "                    print('\\nFraction: {} % < tol: {} %'.format(np.round(delta_label*100, 3), tol))\n",
        "                    print('Reached tolerance threshold. Stopping training.')\n",
        "                    logfile.close()\n",
        "                    break\n",
        "\n",
        "            # train on batch\n",
        "            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
        "            self.model.train_on_batch(x=x[idx], y=p[idx])\n",
        "            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
        "\n",
        "            ite += 1\n",
        "\n",
        "        logfile.close()\n",
        "\n",
        "        if save_dir is not None:\n",
        "            self.model.save_weights(save_dir + '/final.h5')\n",
        "            print(\"Final model saved to: {}/final.h5\".format(save_dir))\n",
        "        return self.predict(x)\n"
      ],
      "metadata": {
        "id": "qrntYh6QI3ZV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWO1awZ9XZ10"
      },
      "outputs": [],
      "source": [
        "! chmod +x test.sh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xXzKXOLwxLL"
      },
      "source": [
        "Movies_Training_Step3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "98mX71EThuA5",
        "outputId": "d42b6bac-3e72-464c-92f7-3b3c831487f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/WeSTClass\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d79841b6976e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/WeSTClass/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/WeSTClass/gen.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspherecluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSphericalKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspherecluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mseed_expansion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_sup_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_sup_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spherecluster/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mspherical_kmeans\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSphericalKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvon_mises_fisher_mixture\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVonMisesFisherMixture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msample_vMF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spherecluster/von_mises_fisher_mixture.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClusterMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_means_\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_init_centroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_tolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_validate_center_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cluster.k_means_'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/WeSTClass/\n",
        "\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIyIFioVI579"
      },
      "outputs": [],
      "source": [
        "#alpha:  Background Word Distribution Weight, it plays a role in generating psudeo documents considering the\n",
        "# amount of topic words and back-ground words used. Having a low alpha results in focusing only on topic-words neglecting the background content and can also overfit the model\n",
        "#ideal range can be vary around 0.85."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ0ntF3sXK6U",
        "outputId": "6d761d95-730f-4cab-f8a0-3183b4a110f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-10-30 03:11:59.317053: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-30 03:11:59.317151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-30 03:11:59.317208: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-30 03:11:59.333109: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-30 03:12:00.744413: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Namespace(dataset='Movies', model='cnn', sup_source='labels', with_evaluation='True', batch_size=256, maxiter=5000.0, pretrain_epochs=None, update_interval=None, alpha=0.85, beta=1000, gamma=100, delta=0.2, trained_weights=None)\n",
            "\n",
            "### Dataset statistics: ###\n",
            "Document max length: 2736 (words)\n",
            "Document average length: 266.50798 (words)\n",
            "Document length std: 196.53431714669986 (words)\n",
            "Defined maximum document length: 500 (words)\n",
            "Fraction of truncated documents: 0.10846\n",
            "Number of classes: 2\n",
            "Number of documents in each class:\n",
            "Class 0: 49998\n",
            "Class 1: 2\n",
            "Vocabulary Size: 112773\n",
            "\n",
            "### Supervision type: Label Surface Names ###\n",
            "Label Names for each class: \n",
            "Supervision content of class 0:\n",
            "['bad']\n",
            "Supervision content of class 1:\n",
            "['good']\n",
            "\n",
            "### Input preparation ###\n",
            "Loading existing Word2Vec model ./Movies/embedding...\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "\n",
            "### Phase 1: vMF distribution fitting & pseudo document generation ###\n",
            "Retrieving top-t nearest words...\n",
            "Final expansion size t = 3\n",
            "Top-t nearest words for each class:\n",
            "Class 0:\n",
            "['bad', 'horrible', 'terrible']\n",
            "Class 1:\n",
            "['good', 'decent', 'great']\n",
            "Finished vMF distribution fitting.\n",
            "[[85]\n",
            " [57]]\n",
            "Pseudo documents generation...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/WeSTClass/main.py\", line 210, in <module>\n",
            "    seed_docs, seed_label = pseudodocs(word_sup_array, gamma, background_array,\n",
            "  File \"/content/drive/MyDrive/WeSTClass/gen.py\", line 106, in pseudodocs\n",
            "    sorted_idx = np.argsort(prob_vec)[::-1]\n",
            "  File \"<__array_function__ internals>\", line 180, in argsort\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\", line 1120, in argsort\n",
            "    return _wrapfunc(a, 'argsort', axis=axis, kind=kind, order=order)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\", line 57, in _wrapfunc\n",
            "    return bound(*args, **kwds)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!./test.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylXXFAOONrqT",
        "outputId": "dec864a1-944b-41e9-9c2c-440f71156895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/WeSTClass/Movies\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.7610619469026548"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/WeSTClass/Movies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "y_pred = []\n",
        "with open(\"out.txt\",\"r\") as pred:\n",
        "  line = pred.readlines()\n",
        "  for i in line:\n",
        "    y_pred.append(int(i))\n",
        "data = np.array(pd.read_csv(\"Movies_Val.csv\", header = None))\n",
        "y= np.array(data[:,0], dtype = np.int8)\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1 = f1_score(y, y_pred[:100])\n",
        "f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyiOQfTxsvQV"
      },
      "outputs": [],
      "source": [
        "input_path = '/content/drive/MyDrive/WeSTClass/Movies/out.txt'\n",
        "output_path = '/content/drive/MyDrive/WeSTClass/Movies/test_pred_movie.txt'\n",
        "\n",
        "# Read the documents from the input file\n",
        "with open(input_path, 'r', encoding='utf-8') as file:\n",
        "    documents = file.readlines()\n",
        "\n",
        "# Extract the last 1000 documents\n",
        "last_1000_docs = documents[-25000:]\n",
        "\n",
        "# Save the last 1000 documents to the output file\n",
        "with open(output_path, 'w', encoding='utf-8') as file:\n",
        "    file.writelines(last_1000_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62DYtwTfNrsy",
        "outputId": "7e63024b-909d-44d3-d0ab-967d930300d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
            "  warnings.warn(\n",
            "WARNING:gensim.models.keyedvectors:attribute count not present in KeyedVectors<vector_size=100, 50110 keys>; will store in internal index_to_key order\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "embedding_file = \"/content/drive/MyDrive/CatE/datasets/News/emb_category_w.txt\"\n",
        "word_vectors = KeyedVectors(vector_size=100)\n",
        "with open(embedding_file, 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    parts = line.split()\n",
        "    word = parts[0]\n",
        "    vector = np.array([float(x) for x in parts[1:]], dtype=np.float32)\n",
        "    word_vectors.add_vector(word, vector)\n",
        "word_vectors.save_word2vec_format(\"custom_word2vec_format.bin\", binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udM2vGoeNrxB",
        "outputId": "5327bac3-435f-431f-df0e-384e0e0ddb12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/WeSTClass\n",
            "2023-10-28 07:17:47.416824: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-28 07:17:47.416918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-28 07:17:47.416977: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-28 07:17:47.431036: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-28 07:17:48.952229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Namespace(dataset='Movies', model='cnn', sup_source='labels', with_evaluation='True', batch_size=256, maxiter=6000.0, pretrain_epochs=None, update_interval=None, alpha=0.85, beta=1000, gamma=100, delta=0.2, trained_weights=None)\n",
            "\n",
            "### Dataset statistics: ###\n",
            "Document max length: 2736 (words)\n",
            "Document average length: 269.483 (words)\n",
            "Document length std: 199.20614456135635 (words)\n",
            "Defined maximum document length: 500 (words)\n",
            "Fraction of truncated documents: 0.11216\n",
            "Number of classes: 2\n",
            "Number of documents in each class:\n",
            "Class 0: 24952\n",
            "Class 1: 48\n",
            "Vocabulary Size: 81470\n",
            "\n",
            "### Supervision type: Label Surface Names ###\n",
            "Label Names for each class: \n",
            "Supervision content of class 0:\n",
            "['bad']\n",
            "Supervision content of class 1:\n",
            "['good']\n",
            "\n",
            "### Input preparation ###\n",
            "Loading existing Word2Vec model ./Movies/custom_word2vec_format.bin...\n",
            "[[-0.15424027  0.06105439 -0.03113613 ...  0.15960103 -0.22144218\n",
            "   0.08471087]\n",
            " [ 0.68099701  1.51282203 -0.62889802 ... -0.084526   -1.80884004\n",
            "   0.083483  ]\n",
            " [ 0.39823899  1.18311906 -0.109412   ...  0.175437   -1.42727304\n",
            "  -0.173271  ]\n",
            " ...\n",
            " [ 0.22674372  0.10905203  0.14802111 ... -0.22537116  0.17161842\n",
            "  -0.11211777]\n",
            " [ 0.01190869 -0.05326586 -0.05127133 ... -0.11520751  0.24426038\n",
            "   0.05722926]\n",
            " [ 0.08269468 -0.01548638 -0.06482986 ... -0.21836762  0.21301507\n",
            "  -0.13665148]]\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "\n",
            "### Phase 1: vMF distribution fitting & pseudo document generation ###\n",
            "Retrieving top-t nearest words...\n",
            "Final expansion size t = 4\n",
            "Top-t nearest words for each class:\n",
            "Class 0:\n",
            "['bad', 'awful', 'terrible', 'horrible']\n",
            "Class 1:\n",
            "['good', 'great', 'but', 'really']\n",
            "Finished vMF distribution fitting.\n",
            "Pseudo documents generation...\n",
            "Finished Pseudo documents generation.\n",
            "\n",
            "### Phase 2: pre-training with pseudo documents ###\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "\n",
            "Neural model summary: \n",
            "Model: \"classifier\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input (InputLayer)          [(None, 500)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 500, 100)             8147000   ['input[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 499, 20)              4020      ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)           (None, 498, 20)              6020      ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 497, 20)              8020      ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, 496, 20)              10020     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d (Glob  (None, 20)                   0         ['conv1d[0][0]']              \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Gl  (None, 20)                   0         ['conv1d_1[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Gl  (None, 20)                   0         ['conv1d_2[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Gl  (None, 20)                   0         ['conv1d_3[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 80)                   0         ['global_max_pooling1d[0][0]',\n",
            "                                                                     'global_max_pooling1d_1[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'global_max_pooling1d_2[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'global_max_pooling1d_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 20)                   1620      ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 2)                    42        ['dense[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8176742 (31.19 MB)\n",
            "Trainable params: 29742 (116.18 KB)\n",
            "Non-trainable params: 8147000 (31.08 MB)\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "Pretraining...\n",
            "Epoch 1/30\n",
            "8/8 [==============================] - 7s 725ms/step - loss: 0.0099\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.0024\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 5s 643ms/step - loss: 0.0013\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 7s 835ms/step - loss: 7.2168e-04\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 7s 845ms/step - loss: 5.5574e-04\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 8s 1s/step - loss: 4.0876e-04\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 8s 932ms/step - loss: 3.1136e-04\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 6s 672ms/step - loss: 2.6655e-04\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 8s 987ms/step - loss: 2.2284e-04\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 6s 716ms/step - loss: 1.9401e-04\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 6s 763ms/step - loss: 1.8086e-04\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 8s 915ms/step - loss: 1.5753e-04\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 6s 704ms/step - loss: 1.4134e-04\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.2329e-04\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 6s 730ms/step - loss: 1.1365e-04\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 8s 972ms/step - loss: 1.0207e-04\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 6s 752ms/step - loss: 9.4048e-05\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 5s 657ms/step - loss: 8.6108e-05\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 8s 1s/step - loss: 7.9275e-05\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 5s 653ms/step - loss: 7.2266e-05\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 7s 881ms/step - loss: 6.6627e-05\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 6s 752ms/step - loss: 6.1371e-05\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 6s 706ms/step - loss: 5.7308e-05\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 8s 1s/step - loss: 5.3546e-05\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 5s 677ms/step - loss: 4.9858e-05\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 7s 870ms/step - loss: 4.7310e-05\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 6s 754ms/step - loss: 4.3464e-05\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 5s 656ms/step - loss: 4.0675e-05\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 8s 1s/step - loss: 3.9082e-05\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 5s 664ms/step - loss: 3.7116e-05\n",
            "Pretraining time: 202.86s\n",
            "Pretrained model saved to ./results/Movies/cnn/phase2/pretrained.h5\n",
            "F1 score after pre-training: f1_macro = 0.07452, f1_micro = 0.07988\n",
            "\n",
            "### Phase 3: self-training ###\n",
            "Update interval: 50\n",
            "782/782 [==============================] - 28s 35ms/step\n",
            "\n",
            "Iter 0: f1_macro = 0.07452, f1_micro = 0.07988\n",
            "Fraction of documents with label changes: 0.0 %\n",
            "\n",
            "Iter 50: f1_macro = 0.0844, f1_micro = 0.09144\n",
            "Fraction of documents with label changes: 1.156 %\n",
            "\n",
            "Iter 100: f1_macro = 0.09764, f1_micro = 0.10732\n",
            "Fraction of documents with label changes: 1.588 %\n",
            "\n",
            "Iter 150: f1_macro = 0.11207, f1_micro = 0.12516\n",
            "Fraction of documents with label changes: 1.784 %\n",
            "\n",
            "Iter 200: f1_macro = 0.12818, f1_micro = 0.14576\n",
            "Fraction of documents with label changes: 2.06 %\n",
            "\n",
            "Iter 250: f1_macro = 0.14281, f1_micro = 0.16516\n",
            "Fraction of documents with label changes: 1.948 %\n",
            "\n",
            "Iter 300: f1_macro = 0.15787, f1_micro = 0.1858\n",
            "Fraction of documents with label changes: 2.064 %\n",
            "\n",
            "Iter 350: f1_macro = 0.17069, f1_micro = 0.20396\n",
            "Fraction of documents with label changes: 1.816 %\n",
            "\n",
            "Iter 400: f1_macro = 0.1839, f1_micro = 0.22324\n",
            "Fraction of documents with label changes: 1.928 %\n",
            "\n",
            "Iter 450: f1_macro = 0.19397, f1_micro = 0.23836\n",
            "Fraction of documents with label changes: 1.512 %\n",
            "\n",
            "Iter 500: f1_macro = 0.20568, f1_micro = 0.25648\n",
            "Fraction of documents with label changes: 1.82 %\n",
            "\n",
            "Iter 550: f1_macro = 0.21407, f1_micro = 0.2698\n",
            "Fraction of documents with label changes: 1.34 %\n",
            "\n",
            "Iter 600: f1_macro = 0.22299, f1_micro = 0.2842\n",
            "Fraction of documents with label changes: 1.44 %\n",
            "\n",
            "Iter 650: f1_macro = 0.22879, f1_micro = 0.29376\n",
            "Fraction of documents with label changes: 0.956 %\n",
            "\n",
            "Iter 700: f1_macro = 0.2362, f1_micro = 0.30616\n",
            "Fraction of documents with label changes: 1.24 %\n",
            "\n",
            "Iter 750: f1_macro = 0.24098, f1_micro = 0.31428\n",
            "Fraction of documents with label changes: 0.812 %\n",
            "\n",
            "Iter 800: f1_macro = 0.24752, f1_micro = 0.32556\n",
            "Fraction of documents with label changes: 1.128 %\n",
            "\n",
            "Iter 850: f1_macro = 0.25148, f1_micro = 0.33248\n",
            "Fraction of documents with label changes: 0.692 %\n",
            "\n",
            "Iter 900: f1_macro = 0.25825, f1_micro = 0.34448\n",
            "Fraction of documents with label changes: 1.2 %\n",
            "\n",
            "Iter 950: f1_macro = 0.26143, f1_micro = 0.3502\n",
            "Fraction of documents with label changes: 0.572 %\n",
            "\n",
            "Iter 1000: f1_macro = 0.26663, f1_micro = 0.35972\n",
            "Fraction of documents with label changes: 0.96 %\n",
            "\n",
            "Iter 1050: f1_macro = 0.26935, f1_micro = 0.36472\n",
            "Fraction of documents with label changes: 0.508 %\n",
            "\n",
            "Iter 1100: f1_macro = 0.27404, f1_micro = 0.3734\n",
            "Fraction of documents with label changes: 0.868 %\n",
            "\n",
            "Iter 1150: f1_macro = 0.27626, f1_micro = 0.37756\n",
            "Fraction of documents with label changes: 0.416 %\n",
            "\n",
            "Iter 1200: f1_macro = 0.28006, f1_micro = 0.38472\n",
            "Fraction of documents with label changes: 0.716 %\n",
            "\n",
            "Iter 1250: f1_macro = 0.28194, f1_micro = 0.3884\n",
            "Fraction of documents with label changes: 0.376 %\n",
            "\n",
            "Iter 1300: f1_macro = 0.28593, f1_micro = 0.39604\n",
            "Fraction of documents with label changes: 0.764 %\n",
            "\n",
            "Iter 1350: f1_macro = 0.28798, f1_micro = 0.4\n",
            "Fraction of documents with label changes: 0.412 %\n",
            "\n",
            "Iter 1400: f1_macro = 0.29113, f1_micro = 0.40612\n",
            "Fraction of documents with label changes: 0.612 %\n",
            "\n",
            "Iter 1450: f1_macro = 0.29332, f1_micro = 0.4104\n",
            "Fraction of documents with label changes: 0.444 %\n",
            "\n",
            "Iter 1500: f1_macro = 0.29676, f1_micro = 0.4172\n",
            "Fraction of documents with label changes: 0.688 %\n",
            "\n",
            "Iter 1550: f1_macro = 0.29861, f1_micro = 0.42088\n",
            "Fraction of documents with label changes: 0.384 %\n",
            "\n",
            "Iter 1600: f1_macro = 0.30109, f1_micro = 0.42584\n",
            "Fraction of documents with label changes: 0.496 %\n",
            "\n",
            "Iter 1650: f1_macro = 0.30306, f1_micro = 0.4298\n",
            "Fraction of documents with label changes: 0.412 %\n",
            "\n",
            "Iter 1700: f1_macro = 0.30524, f1_micro = 0.4342\n",
            "Fraction of documents with label changes: 0.448 %\n",
            "\n",
            "Iter 1750: f1_macro = 0.30706, f1_micro = 0.43792\n",
            "Fraction of documents with label changes: 0.38 %\n",
            "\n",
            "Iter 1800: f1_macro = 0.30873, f1_micro = 0.44132\n",
            "Fraction of documents with label changes: 0.356 %\n",
            "\n",
            "Iter 1850: f1_macro = 0.3106, f1_micro = 0.44528\n",
            "Fraction of documents with label changes: 0.404 %\n",
            "\n",
            "Iter 1900: f1_macro = 0.31222, f1_micro = 0.44864\n",
            "Fraction of documents with label changes: 0.336 %\n",
            "\n",
            "Iter 1950: f1_macro = 0.31382, f1_micro = 0.45196\n",
            "Fraction of documents with label changes: 0.34 %\n",
            "\n",
            "Iter 2000: f1_macro = 0.31496, f1_micro = 0.45432\n",
            "Fraction of documents with label changes: 0.244 %\n",
            "\n",
            "Iter 2050: f1_macro = 0.31687, f1_micro = 0.45832\n",
            "Fraction of documents with label changes: 0.408 %\n",
            "\n",
            "Iter 2100: f1_macro = 0.31782, f1_micro = 0.46044\n",
            "Fraction of documents with label changes: 0.268 %\n",
            "\n",
            "Iter 2150: f1_macro = 0.31939, f1_micro = 0.46376\n",
            "Fraction of documents with label changes: 0.34 %\n",
            "\n",
            "Iter 2200: f1_macro = 0.31975, f1_micro = 0.46452\n",
            "Fraction of documents with label changes: 0.212 %\n",
            "\n",
            "Iter 2250: f1_macro = 0.32166, f1_micro = 0.46856\n",
            "Fraction of documents with label changes: 0.42 %\n",
            "\n",
            "Iter 2300: f1_macro = 0.32171, f1_micro = 0.46868\n",
            "Fraction of documents with label changes: 0.22 %\n",
            "\n",
            "Iter 2350: f1_macro = 0.32337, f1_micro = 0.4722\n",
            "Fraction of documents with label changes: 0.368 %\n",
            "\n",
            "Iter 2400: f1_macro = 0.32332, f1_micro = 0.47224\n",
            "Fraction of documents with label changes: 0.26 %\n",
            "\n",
            "Iter 2450: f1_macro = 0.32515, f1_micro = 0.47616\n",
            "Fraction of documents with label changes: 0.408 %\n",
            "\n",
            "Iter 2500: f1_macro = 0.32493, f1_micro = 0.47568\n",
            "Fraction of documents with label changes: 0.208 %\n",
            "\n",
            "Iter 2550: f1_macro = 0.32638, f1_micro = 0.4788\n",
            "Fraction of documents with label changes: 0.32 %\n",
            "\n",
            "Iter 2600: f1_macro = 0.32573, f1_micro = 0.4774\n",
            "Fraction of documents with label changes: 0.22 %\n",
            "\n",
            "Iter 2650: f1_macro = 0.32732, f1_micro = 0.48084\n",
            "Fraction of documents with label changes: 0.36 %\n",
            "\n",
            "Iter 2700: f1_macro = 0.32647, f1_micro = 0.479\n",
            "Fraction of documents with label changes: 0.232 %\n",
            "\n",
            "Iter 2750: f1_macro = 0.32816, f1_micro = 0.48264\n",
            "Fraction of documents with label changes: 0.436 %\n",
            "\n",
            "Iter 2800: f1_macro = 0.32703, f1_micro = 0.4802\n",
            "Fraction of documents with label changes: 0.292 %\n",
            "\n",
            "Iter 2850: f1_macro = 0.32853, f1_micro = 0.48344\n",
            "Fraction of documents with label changes: 0.34 %\n",
            "\n",
            "Iter 2900: f1_macro = 0.32723, f1_micro = 0.48064\n",
            "Fraction of documents with label changes: 0.376 %\n",
            "\n",
            "Iter 2950: f1_macro = 0.32893, f1_micro = 0.48432\n",
            "Fraction of documents with label changes: 0.384 %\n",
            "\n",
            "Iter 3000: f1_macro = 0.32799, f1_micro = 0.48228\n",
            "Fraction of documents with label changes: 0.268 %\n",
            "\n",
            "Iter 3050: f1_macro = 0.32978, f1_micro = 0.48616\n",
            "Fraction of documents with label changes: 0.404 %\n",
            "\n",
            "Iter 3100: f1_macro = 0.32873, f1_micro = 0.48388\n",
            "Fraction of documents with label changes: 0.252 %\n",
            "\n",
            "Iter 3150: f1_macro = 0.33055, f1_micro = 0.48784\n",
            "Fraction of documents with label changes: 0.396 %\n",
            "\n",
            "Iter 3200: f1_macro = 0.32932, f1_micro = 0.48516\n",
            "Fraction of documents with label changes: 0.316 %\n",
            "\n",
            "Iter 3250: f1_macro = 0.33132, f1_micro = 0.48952\n",
            "Fraction of documents with label changes: 0.46 %\n",
            "\n",
            "Iter 3300: f1_macro = 0.33029, f1_micro = 0.48728\n",
            "Fraction of documents with label changes: 0.248 %\n",
            "\n",
            "Iter 3350: f1_macro = 0.33158, f1_micro = 0.49024\n",
            "Fraction of documents with label changes: 0.32 %\n",
            "\n",
            "Iter 3400: f1_macro = 0.33073, f1_micro = 0.48824\n",
            "Fraction of documents with label changes: 0.296 %\n",
            "\n",
            "Iter 3450: f1_macro = 0.33224, f1_micro = 0.49168\n",
            "Fraction of documents with label changes: 0.368 %\n",
            "\n",
            "Iter 3500: f1_macro = 0.33175, f1_micro = 0.4906\n",
            "Fraction of documents with label changes: 0.292 %\n",
            "\n",
            "Iter 3550: f1_macro = 0.33277, f1_micro = 0.49284\n",
            "Fraction of documents with label changes: 0.264 %\n",
            "\n",
            "Iter 3600: f1_macro = 0.33237, f1_micro = 0.49196\n",
            "Fraction of documents with label changes: 0.216 %\n",
            "\n",
            "Iter 3650: f1_macro = 0.33349, f1_micro = 0.49428\n",
            "Fraction of documents with label changes: 0.264 %\n",
            "\n",
            "Iter 3700: f1_macro = 0.33318, f1_micro = 0.4936\n",
            "Fraction of documents with label changes: 0.156 %\n",
            "\n",
            "Fraction: 0.156 % < tol: 0.2 %\n",
            "Reached tolerance threshold. Stopping training.\n",
            "Final model saved to: ./results/Movies/cnn/phase3/final.h5\n",
            "Self-training time: 5588.09s\n",
            "\n",
            "### Generating outputs ###\n",
            "Classification results are written in ./Movies/out.txt\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/WeSTClass/\n",
        "!./test.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tELMnlEbNry7",
        "outputId": "9b60f9a9-c5e9-47b4-a90e-077b1a7147c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/WeSTClass/Movies\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.787878787878788"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/WeSTClass/Movies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "y_pred = []\n",
        "with open(\"out.txt\",\"r\") as pred:\n",
        "  line = pred.readlines()\n",
        "  for i in line:\n",
        "    y_pred.append(int(i))\n",
        "data = np.array(pd.read_csv(\"Movies_Val.csv\", header = None))\n",
        "y= np.array(data[:,0], dtype = np.int8)\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1 = f1_score(y, y_pred[:100])\n",
        "f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFzXhuop49-z",
        "outputId": "a33ac1ee-109f-4198-8500-7d7c9948f05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUotkpBUKUst",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "f8c7b2b1070c438ab71c8c73c5f2cfed",
            "9a2ed8b8ff10424ea053ca80071c26a0",
            "59d685b3e07b4b2a8406b5c7e1f0db01",
            "ca31ddef84f94ed4b1231c2d3a60c211",
            "d6dc7dbe03d24be7b815d47010b7e61b",
            "fcc471a563e64c9e914502fb1f1d3ec7",
            "860047a7bc6a4ced8c81b0f80320913d",
            "ac23822c670d4657af5ee82c3726b736",
            "41012ab3500d4cb9afac37e5f63e6ea3",
            "dc5f94498bae49caa5fa699edfc68e22",
            "ee194a772c394465a46458d6041ce213",
            "b471c8b35b634ca48c2ca87208a3789d",
            "c262cfb4561746eeada8b2ac43b951ff",
            "d15de6058dd8412a9664cc2186ad8372",
            "d424bc57968f4e5d83cb83099bea15f3",
            "4c9a1ef8a63547a389a145736481a8d1",
            "3a39604339ac4a7aa224f77f0650044c",
            "6e132e264aa74e07bd184b6112dcac73",
            "f79cb883c6c64954b34790f64b2e3131",
            "25be496d077940788a6b57f8f1dc9705",
            "3b7339cab2064db9a6e16e9c79309545",
            "7d70be902d8a400e915c02bf65cd6e11",
            "ee365660a78440f5a7a6b56655bd9df7",
            "feb45795541a4cf180af0eb23fc959eb",
            "b694c1e013ac425cb943f6f415752d8a",
            "5c4c2a4a0f2446f49bd37c4ab6d43077",
            "580399199fc24d5da20dfb45c5c89685",
            "b4b269bc840b43738dfb2aa5b7e08a69",
            "6680337df3f64aa4bf4d746050c41a01",
            "dbd4c3bca4be42a89487572983be6e79",
            "d4fc4c3964684625a2fddac86da8d352",
            "c8ea27f36747489787be90f8be838f05",
            "5c0a473f7177485aa34a8404d2fd7e17",
            "73f8317d74114170afb577641f84d62e",
            "5125831ed3e3468eb089f94d81c12414",
            "e9da3fcc21cd48aa91a6c80b9f668126",
            "5c7aff02a52f410cab158ee06e674fc1",
            "ba134fffa8df4f4e875d169ea5a1941b",
            "030642fc850943f6a43ffb8012db4383",
            "f00a58ac12e249539a0fa0fda0ca4f92",
            "d04779ffff0b46ce99a06245ce9f6c53",
            "1b6ee9bcb875485ea47a9fe0cc64223e",
            "1b162e2304114e399ffdacd31fcf55aa",
            "c9ed0dcfa25d40fe9839191a0234c453",
            "36a708f0b2a34e12bf8f0d4913c9dcb8",
            "e8935e212e5143ad9dc8b777f5d7eefd",
            "04e24a0c7f7745d28f4fca707d1f7bf3",
            "2ea47222810f46c0a196eb933f4f7220",
            "6fc5ce4b26fe47e2b6037a2e3d1562ad",
            "56e464559e53431fb328d7886ba438ca",
            "409355d0c97a48418c2448354ba326c1",
            "f891d16c72d743abbb8a12c1869b6dd9",
            "6b58f5df214c4a6fab55765eeecadeb8",
            "29599b998083498094ab545d89630720",
            "036b3f00acb148ba9f13e2f91e306144",
            "0d756a6875d842e9841ab186240ed797",
            "4400de32d6bf4bc3ad2da06a675852c0",
            "393742c9ddd9446dad8d03a30a39f42b",
            "0eacbbf0ff834db28ce83755f3f332f4",
            "65cb89850c9a41d3984a6cf5d88cb1e1",
            "6d57d753ef714ec2ac8560983042e568",
            "76ae672cd9214263bf8954266188498d",
            "7125eafef4954d62a1b6f4f25613ffdb",
            "ca307f2f6e2144249ea2aefea04c7133",
            "6823f180859c4e2ca8a6ab5c81f44e2e",
            "ec088985e91b48fcb3f8573c7d80fa7c"
          ]
        },
        "outputId": "3d8c4d84-e7a1-4fc6-829f-c2492e920af0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8c7b2b1070c438ab71c8c73c5f2cfed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b471c8b35b634ca48c2ca87208a3789d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee365660a78440f5a7a6b56655bd9df7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73f8317d74114170afb577641f84d62e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36a708f0b2a34e12bf8f0d4913c9dcb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d756a6875d842e9841ab186240ed797"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model = \"facebook/bart-large-mnli\")\n",
        "labels = [\"politics\",\"sports\",\"business\",\"technology\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOFQ-wrjKUvT",
        "outputId": "68aaca23-e576-4aa8-dd84-63185fc4427f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-2edb653bfbb9>:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  data = pd.read_csv(\"/content/drive/MyDrive/WeSTClass/News/combined_file.csv\", header = None,error_bad_lines=False)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/WeSTClass/News/combined_file.csv\", header = None,error_bad_lines=False)\n",
        "data.columns = [\"actual_labels\",\"document\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeJ3buuRKUzz",
        "outputId": "bce5b714-336f-45fd-a19c-ee6c7f153c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [2:27:52<00:00,  4.44s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "for i in tqdm(range(4000,6000)):\n",
        "  sequence= data.iloc[i,1]\n",
        "  data.loc[i,\"pseudo_label_bart-l-m\"]=classifier(sequence,labels)[\"labels\"][0]\n",
        "data.to_csv(\"/content/drive/MyDrive/WeSTClass/News/combined_file_latest5.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8c7b2b1070c438ab71c8c73c5f2cfed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a2ed8b8ff10424ea053ca80071c26a0",
              "IPY_MODEL_59d685b3e07b4b2a8406b5c7e1f0db01",
              "IPY_MODEL_ca31ddef84f94ed4b1231c2d3a60c211"
            ],
            "layout": "IPY_MODEL_d6dc7dbe03d24be7b815d47010b7e61b"
          }
        },
        "9a2ed8b8ff10424ea053ca80071c26a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcc471a563e64c9e914502fb1f1d3ec7",
            "placeholder": "​",
            "style": "IPY_MODEL_860047a7bc6a4ced8c81b0f80320913d",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "59d685b3e07b4b2a8406b5c7e1f0db01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac23822c670d4657af5ee82c3726b736",
            "max": 1154,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41012ab3500d4cb9afac37e5f63e6ea3",
            "value": 1154
          }
        },
        "ca31ddef84f94ed4b1231c2d3a60c211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc5f94498bae49caa5fa699edfc68e22",
            "placeholder": "​",
            "style": "IPY_MODEL_ee194a772c394465a46458d6041ce213",
            "value": " 1.15k/1.15k [00:00&lt;00:00, 15.9kB/s]"
          }
        },
        "d6dc7dbe03d24be7b815d47010b7e61b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcc471a563e64c9e914502fb1f1d3ec7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "860047a7bc6a4ced8c81b0f80320913d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac23822c670d4657af5ee82c3726b736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41012ab3500d4cb9afac37e5f63e6ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc5f94498bae49caa5fa699edfc68e22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee194a772c394465a46458d6041ce213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b471c8b35b634ca48c2ca87208a3789d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c262cfb4561746eeada8b2ac43b951ff",
              "IPY_MODEL_d15de6058dd8412a9664cc2186ad8372",
              "IPY_MODEL_d424bc57968f4e5d83cb83099bea15f3"
            ],
            "layout": "IPY_MODEL_4c9a1ef8a63547a389a145736481a8d1"
          }
        },
        "c262cfb4561746eeada8b2ac43b951ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a39604339ac4a7aa224f77f0650044c",
            "placeholder": "​",
            "style": "IPY_MODEL_6e132e264aa74e07bd184b6112dcac73",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "d15de6058dd8412a9664cc2186ad8372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f79cb883c6c64954b34790f64b2e3131",
            "max": 1629437147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25be496d077940788a6b57f8f1dc9705",
            "value": 1629437147
          }
        },
        "d424bc57968f4e5d83cb83099bea15f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b7339cab2064db9a6e16e9c79309545",
            "placeholder": "​",
            "style": "IPY_MODEL_7d70be902d8a400e915c02bf65cd6e11",
            "value": " 1.63G/1.63G [00:15&lt;00:00, 173MB/s]"
          }
        },
        "4c9a1ef8a63547a389a145736481a8d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a39604339ac4a7aa224f77f0650044c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e132e264aa74e07bd184b6112dcac73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f79cb883c6c64954b34790f64b2e3131": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25be496d077940788a6b57f8f1dc9705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b7339cab2064db9a6e16e9c79309545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d70be902d8a400e915c02bf65cd6e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee365660a78440f5a7a6b56655bd9df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_feb45795541a4cf180af0eb23fc959eb",
              "IPY_MODEL_b694c1e013ac425cb943f6f415752d8a",
              "IPY_MODEL_5c4c2a4a0f2446f49bd37c4ab6d43077"
            ],
            "layout": "IPY_MODEL_580399199fc24d5da20dfb45c5c89685"
          }
        },
        "feb45795541a4cf180af0eb23fc959eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4b269bc840b43738dfb2aa5b7e08a69",
            "placeholder": "​",
            "style": "IPY_MODEL_6680337df3f64aa4bf4d746050c41a01",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "b694c1e013ac425cb943f6f415752d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbd4c3bca4be42a89487572983be6e79",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4fc4c3964684625a2fddac86da8d352",
            "value": 26
          }
        },
        "5c4c2a4a0f2446f49bd37c4ab6d43077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8ea27f36747489787be90f8be838f05",
            "placeholder": "​",
            "style": "IPY_MODEL_5c0a473f7177485aa34a8404d2fd7e17",
            "value": " 26.0/26.0 [00:00&lt;00:00, 1.07kB/s]"
          }
        },
        "580399199fc24d5da20dfb45c5c89685": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4b269bc840b43738dfb2aa5b7e08a69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6680337df3f64aa4bf4d746050c41a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbd4c3bca4be42a89487572983be6e79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4fc4c3964684625a2fddac86da8d352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8ea27f36747489787be90f8be838f05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c0a473f7177485aa34a8404d2fd7e17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73f8317d74114170afb577641f84d62e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5125831ed3e3468eb089f94d81c12414",
              "IPY_MODEL_e9da3fcc21cd48aa91a6c80b9f668126",
              "IPY_MODEL_5c7aff02a52f410cab158ee06e674fc1"
            ],
            "layout": "IPY_MODEL_ba134fffa8df4f4e875d169ea5a1941b"
          }
        },
        "5125831ed3e3468eb089f94d81c12414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_030642fc850943f6a43ffb8012db4383",
            "placeholder": "​",
            "style": "IPY_MODEL_f00a58ac12e249539a0fa0fda0ca4f92",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "e9da3fcc21cd48aa91a6c80b9f668126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d04779ffff0b46ce99a06245ce9f6c53",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b6ee9bcb875485ea47a9fe0cc64223e",
            "value": 898822
          }
        },
        "5c7aff02a52f410cab158ee06e674fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b162e2304114e399ffdacd31fcf55aa",
            "placeholder": "​",
            "style": "IPY_MODEL_c9ed0dcfa25d40fe9839191a0234c453",
            "value": " 899k/899k [00:00&lt;00:00, 31.7MB/s]"
          }
        },
        "ba134fffa8df4f4e875d169ea5a1941b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "030642fc850943f6a43ffb8012db4383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f00a58ac12e249539a0fa0fda0ca4f92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d04779ffff0b46ce99a06245ce9f6c53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b6ee9bcb875485ea47a9fe0cc64223e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b162e2304114e399ffdacd31fcf55aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9ed0dcfa25d40fe9839191a0234c453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36a708f0b2a34e12bf8f0d4913c9dcb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8935e212e5143ad9dc8b777f5d7eefd",
              "IPY_MODEL_04e24a0c7f7745d28f4fca707d1f7bf3",
              "IPY_MODEL_2ea47222810f46c0a196eb933f4f7220"
            ],
            "layout": "IPY_MODEL_6fc5ce4b26fe47e2b6037a2e3d1562ad"
          }
        },
        "e8935e212e5143ad9dc8b777f5d7eefd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56e464559e53431fb328d7886ba438ca",
            "placeholder": "​",
            "style": "IPY_MODEL_409355d0c97a48418c2448354ba326c1",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "04e24a0c7f7745d28f4fca707d1f7bf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f891d16c72d743abbb8a12c1869b6dd9",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b58f5df214c4a6fab55765eeecadeb8",
            "value": 456318
          }
        },
        "2ea47222810f46c0a196eb933f4f7220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29599b998083498094ab545d89630720",
            "placeholder": "​",
            "style": "IPY_MODEL_036b3f00acb148ba9f13e2f91e306144",
            "value": " 456k/456k [00:00&lt;00:00, 16.7MB/s]"
          }
        },
        "6fc5ce4b26fe47e2b6037a2e3d1562ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56e464559e53431fb328d7886ba438ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "409355d0c97a48418c2448354ba326c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f891d16c72d743abbb8a12c1869b6dd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b58f5df214c4a6fab55765eeecadeb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29599b998083498094ab545d89630720": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "036b3f00acb148ba9f13e2f91e306144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d756a6875d842e9841ab186240ed797": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4400de32d6bf4bc3ad2da06a675852c0",
              "IPY_MODEL_393742c9ddd9446dad8d03a30a39f42b",
              "IPY_MODEL_0eacbbf0ff834db28ce83755f3f332f4"
            ],
            "layout": "IPY_MODEL_65cb89850c9a41d3984a6cf5d88cb1e1"
          }
        },
        "4400de32d6bf4bc3ad2da06a675852c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d57d753ef714ec2ac8560983042e568",
            "placeholder": "​",
            "style": "IPY_MODEL_76ae672cd9214263bf8954266188498d",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "393742c9ddd9446dad8d03a30a39f42b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7125eafef4954d62a1b6f4f25613ffdb",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca307f2f6e2144249ea2aefea04c7133",
            "value": 1355863
          }
        },
        "0eacbbf0ff834db28ce83755f3f332f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6823f180859c4e2ca8a6ab5c81f44e2e",
            "placeholder": "​",
            "style": "IPY_MODEL_ec088985e91b48fcb3f8573c7d80fa7c",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 31.2MB/s]"
          }
        },
        "65cb89850c9a41d3984a6cf5d88cb1e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d57d753ef714ec2ac8560983042e568": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76ae672cd9214263bf8954266188498d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7125eafef4954d62a1b6f4f25613ffdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca307f2f6e2144249ea2aefea04c7133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6823f180859c4e2ca8a6ab5c81f44e2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec088985e91b48fcb3f8573c7d80fa7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}